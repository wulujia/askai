+++
date = '2025-02-25T07:08:30+08:00'
draft = false
title = 'TruthfulQA 幻觉检测'
+++

我看到文章里提到：使用 TruthfulQA 等数据集，测试模型在常见问答中是否给出真实的回答。检测“幻觉”（hallucinations）——在有准确答案的问题上，模型是否编造错误信息。

我的问题是：

1. TruthfulQA 是个什么数据集，为什么能检测是否模型给出了真实回答？
2. 检测模型幻觉方面，目前业界有什么通用、成熟或者突破性的做法？
3. 以上内容，是否有论文支撑，如果有，请推荐给我。

> 关于 TruthfulQA：
> 
> - 您希望了解该数据集的构建方式、测试方法，还是主要关注其适用范围？
> 
> 关于幻觉检测方法：
> 
> - 您希望了解当前主流的方法、行业标准，还是近期的突破性研究？
> - 是否需要具体案例或方法论的对比？
> 
> 关于相关论文：
> 
> - 您希望推荐综述性论文（综述多个方法）、特定方法的论文，还是官方介绍 TruthfulQA 的论文？
> - 是否有发布时间范围要求，例如最新的研究？

我考虑：
1. 既关心该数据集的构建方式、测试方法，也关心适用范围。
2. 希望了解当前主流的方法、行业标准，以及近期的突破性研究，需要具体案例或方法论的对比。
3. 综述性论文（综述多个方法）、特定方法的论文，还是官方介绍 TruthfulQA 的论文都可以，最好是 2022 年以后的，再早的可能参考意义不大。

> 好的，我会收集以下内容：
> 
> 1. TruthfulQA 数据集的构建方式、测试方法及适用范围。
> 2. 当前业界主流的模型幻觉检测方法，包括行业标准和近期的突破性研究，并提供具体案例或方法论对比。
> 3. 2022 年及以后的相关论文，包括综述性论文、特定方法的研究论文，以及 TruthfulQA 相关的官方论文。
> 
> 我会尽快整理这些信息，并向您反馈。

# TruthfulQA 数据集：构建方式、测试方法及适用范围

**构建方式：**TruthfulQA 是用于评估语言模型回答是否真实（truthful）的基准数据集，由作者人工编写了 817 道开放问答题，涵盖健康、法律、金融、政治等 38 个类别 ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=We%20propose%20a%20benchmark%20to,The%20largest%20models%20were))。这些问题设计成**容易引出常见谬误**或人类的错误观念，使得未经特别校准的模型可能根据训练语料模仿人类错误回答 ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=38%20categories%2C%20including%20health%2C%20law%2C,The%20largest%20models%20were))。每个问题都附有若干**正确答案和错误答案的参考**，以及支持正确答案的资料来源（如维基百科页面） ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=TruthfulQA%20consists%20of%20a%20test,reference%20answers%20and%20a%20source))。数据集专门用于**零样本**(zero-shot)评估（不允许在该数据上微调模型），以测试预训练模型在陌生问题上的真实性表现 ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=2,with%20a%20median%20length%20of))。

**测试方法：**TruthfulQA 提供开放式生成和多项选择两种评测形式 ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=9%20words,See%20Appendix%20G%20for%20details))。主要任务是让模型对每道问题生成自由文本答案，然后由人工依据参考答案判定其“真实度”（truthfulness）和信息量 ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=for%20every%20question,we%20discuss%20in%20Section%206)) ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=9%20words,See%20Appendix%20G%20for%20details))。评估者会检查模型答案是否与参考事实一致且无虚假信息，同时评价答案是否有用（informativeness）以防止模型只给出含糊或规避式回答 ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=for%20every%20question,we%20discuss%20in%20Section%206))。由于人工评价成本高，作者还训练了一个**自动判别器 “GPT-judge”**（基于 GPT-3 13B 微调而成）来预测答案真伪，其在判别人类标注上达到约 90–96% 的准确率 ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=proxy%20for%20the%20gold%02standard%20of,The%20training%20set%20for%20GPT)) ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=predicts%20human%20evaluations%20with%20accuracy,qualitative%20features%20of%20Figure%202))。模型在 TruthfulQA 上的表现以“真实回答比例”衡量，例如原始论文报告GPT-3系列等模型最高只有 58% 的回答是真实的，而人类在同样问题上的真实率约为 94% ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=misconception,tuning%20using))。值得注意的是，**较大的模型并不一定更真实**：例如6亿参数的GPT-J比1.25亿参数版本的错误率更高 ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=human%20texts.%20We%20tested%20GPT,tuning%20using))。这与许多NLP任务上模型规模提升带来性能提升的趋势相反，表明仅靠从海量网络文本模仿可能会内化错误信息。因此，作者建议采用除模仿网络文本之外的训练目标（如结合事实校对微调）来提高模型的真实性 ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=least%20truthful,tuning%20using)) ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=T5,of%20text%20from%20the%20web))。

**适用范围：**TruthfulQA 适用于**开放领域问答**和对话场景下评估模型的事实准确性，尤其关注模型是否会复述人类常见谬误 ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=We%20propose%20a%20benchmark%20to,The%20largest%20models%20were))。由于问题涵盖广泛领域且刻意具有迷惑性，它可用于测试大型语言模型在**不依赖外部检索**情况下的知识可靠性和抗诱骗能力。例如，在医疗或法律问答中，使用 TruthfulQA 可以检验模型是否会给出流传甚广但错误的说法。这一基准已成为评估**模型幻觉（hallucination）**倾向的常用指标之一，被AI安全和真实性研究社区广泛引用，用来比较不同模型或训练策略在避免虚假信息方面的效果 ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=misconception,tuning%20using)) ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=misconceptions%20and%20have%20the%20potential,of%20text%20from%20the%20web))。需要注意的是，TruthfulQA集中于常见错误观念导致的谬误，对于需要实时事实更新的知识（如最新资讯）或有上下文支撑的生成任务，可能需要配合其他基准一起评估模型性能。

# 模型幻觉的检测方法：行业现状与最新进展

大型语言模型生成**“幻觉”**是指输出内容在事实或语境上不正确、无根据甚至自相矛盾的现象。这是当前行业部署LLM面临的主要挑战之一 ([A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/html/2311.05232v2#:~:text=hallucinations%20%28Bang%20et%C2%A0al,Given%20that))。以下概述主流的幻觉检测方法以及近期的突破性研究：

## 行业主流检测方法

- **基于知识检索与验证：**许多工业系统通过引入检索模块来减少幻觉，例如先从知识库或互联网检索相关信息，再让模型参考检索结果作答，并核对答案中的事实 ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=Prior%20work%20has%20proposed%20various,effectiveness%20trade))。对于纯生成后的答案，还可使用**事实核对**技术：将生成的断言提取出来，检索证据并利用自然语言推理（NLI）模型或规则判断断言是否与证据一致（类似于FEVER挑战中的方法）。这种方案要求有权威的外部知识源，可被视为**事实一致性**检测，即检查模型输出与外部真实世界知识是否一致 ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=Prior%20work%20has%20proposed%20various,effectiveness%20trade))。很多摘要和对话系统也采用类似办法，将生成内容与原始输入对比，确保关键信息一致。
- **基于指标的匹配与约束：**早期有使用 n-gram 重叠指标（如BLEU/ROUGE）或实体/关系匹配来粗略评估输出与来源的吻合度 ([A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/html/2311.05232v2#:~:text=triples.%20Traditional%20n,overlap%20of%20relation%20tuples%20and))。例如对于摘要，计算生成摘要和原文在实体、关系上的重合来发现丢失或捏造的信息 ([A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/html/2311.05232v2#:~:text=2002%20%29%2C%20ROUGE%20,of%20relation%20tuples%20extracted%20using))。不过这些硬匹配往往不足以捕捉深层不一致 ([A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/html/2311.05232v2#:~:text=facts%2C%20faithfulness%20can%20be%20measured,based%20metrics))。因此一些改进指标（如PARENT-T、QuestEval等）综合了语义匹配和任务上下文，用于参考**无关语境**（如开放问答）的幻觉检测。
- **不依赖参考的模型内评估：**在无法获取外部知识的情况下，业内也探索让模型**自我检查**输出。如 OpenAI 等采用让模型生成答案后，再提示模型或另一个更强模型来评估答案的真实性（要求其说明依据或打分）。类似地，**GPT-judge**在TruthfulQA中的应用就是将一个模型微调成判别器，用模型判断模型 ([](https://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf#:~:text=proxy%20for%20the%20gold%02standard%20of,The%20training%20set%20for%20GPT))。最近，业界出现**让模型自己验证**的方法，例如让模型用不同措辞多次回答同一问题，若答案前后矛盾则可能存在幻觉 ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=based%20approach%20that%20can%20be,our%20approach%20to%20several%20baselines))。同时监测模型**内部不确定性**也是手段之一：例如利用输出的概率分布熵、模型logit差距等作为置信度指标，置信度过低可能意味着模型在胡编乱造 ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=hallucinations%20in%20LLM,world%20applications%20with%20constraints))。还有研究利用模型的注意力权重或隐藏状态，检测其对输入的依赖程度来判断幻觉倾向。
- **组合多种信号：**实践中没有单一指标在所有场景下完胜，其性能往往依赖任务和模型 ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=downstream%20decision%20making,match%20or%20even%20outperform%20more))。因此工业界常结合多种检测信号提升可靠性。例如某些企业方案同时考虑“与知识检索结果的匹配度”、“模型自我一致性”、NLI判别结果等，综合这些得分来判定回答是否可信 ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=employ%20diverse%20LLMs%20to%20ensure,while%20significantly%20reducing%20computational%20overhead))。最新研究也表明，通过**多重打分融合**可以在不同数据集上取得更稳定的检测效果，并可在计算开销和准确率间取得平衡 ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=downstream%20decision%20making,match%20or%20even%20outperform%20more))。这一思路契合实际应用中对成本和风险的权衡需求 ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=hallucination%20detection%20entails%3A%20first%2C%20producing,scoring%20framework%2C%20which%20combines)) ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=employ%20diverse%20LLMs%20to%20ensure,while%20significantly%20reducing%20computational%20overhead))。例如，有工作提出校准各个检测指标的置信度后再融合，以降低误判率，在问答、事实核查、摘要等任务上都获得领先的检测表现 ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=hallucination%20detection%20entails%3A%20first%2C%20producing,scoring%20framework%2C%20which%20combines)) ([Cost-Effective Hallucination Detection for LLMs](https://arxiv.org/html/2407.21424v1#:~:text=employ%20diverse%20LLMs%20to%20ensure,while%20significantly%20reducing%20computational%20overhead))。总体而言，主流方法是将**外部事实验证、模型自身信心、输出一致性**等多种技术结合，以最大程度捕捉幻觉迹象并保证低漏报和低误报率。

## 最新研究进展

- **SelfCheckGPT（自检）方法：**这是 Cambridge 等提出的一种**零资源黑盒**幻觉检测方法（EMNLP 2023） ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=Generative%20Large%20Language%20Models%20,SelfCheckGPT))。它的核心思想是让模型针对同一输入**多次随机采样生成**回答——如果模型对某一事实确有掌握，各次回答应当相似且关键事实一致；反之，如果某部分内容是幻觉捏造的，不同采样往往会产生互相矛盾的说法 ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=in%20a%20zero,our%20approach%20to%20several%20baselines))。通过对这些采样结果的一致性进行分析，SelfCheckGPT 无需访问模型内部概率即可判断输出是否可靠 ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=highly%20fluent%20responses%20to%20a,knowledge%20of%20a%20given%20concept)) ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=such%20as%20ChatGPT,the%20WikiBio%20dataset%2C%20and%20manually))。研究表明，该方法能**有效区分真实语句和虚假语句**，并可根据回答的 factuality 对整段文本排序 ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=However%2C%20for%20hallucinated%20facts%2C%20stochastically,box%20methods))。在句子级幻觉检测任务上，其AUC-PR（基于精确率-召回曲线下的面积）显著高于其它无需外部资源的方法，在段落级的整体真实性评估上与更复杂的灰盒方法有更高的相关性 ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=to%20generate%20passages%20about%20individuals,box%20methods))。这证明利用模型自身的**回答一致性**是检测幻觉的有力信号。
- **RelD 判别器：**这是 Chen 等人在 CIKM 2023 提出的一种**鲁棒幻觉检测**模型 ([[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org/abs/2407.04121#:~:text=hallucination%2C%20where%20they%20generate%20unfaithful,of))。他们构建了一个包含模型生成回答的问答对话数据集（称为RelQA，包括中英双语）以及多种评价指标，用于训练一个名为 RelD 的二分类器 ([[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org/abs/2407.04121#:~:text=we%20propose%20a%20robust%20discriminator,a%20thorough%20analysis%20of%20the))。实验结果显示，RelD 能**有效识别各类大型模型生成答案中的幻觉**，对于训练分布内和分布外的数据都能很好地区分是否存在幻觉 ([[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org/abs/2407.04121#:~:text=LLMs%20and%20a%20comprehensive%20set,hallucination%20in%20the%20future%20work))。此外作者还细致分析了不同类型幻觉的特征，为日后缓解此问题提供了洞见 ([[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org/abs/2407.04121#:~:text=that%20the%20proposed%20RelD%20successfully,hallucination%20in%20the%20future%20work))。这一研究表明通过**专门数据集微调的判别模型**可以提升跨模型、跨领域的幻觉检测准确性。
- **其它方向：**最新一些工作探索让更强大的LLM充当评审来检测较小模型的幻觉，或者引入**链式推理**(Chain-of-Thought)让模型逐步验证每个事实；还有研究将**知识图谱**融入LLM以减少和检测不符合已知知识的输出 ([[PDF] Mitigating Large Language Model Hallucinations via Autonomous ...](https://ojs.aaai.org/index.php/AAAI/article/view/29770/31326#:~:text=,LLMs))。在多模态领域也出现了幻觉检测的数据集和模型（如 M-HalDetect 用于检测图文模型的幻觉 ([Detecting and Preventing Hallucinations in Large Vision Language ...](https://ojs.aaai.org/index.php/AAAI/article/view/29771#:~:text=Detecting%20and%20Preventing%20Hallucinations%20in,for%20hallucination%20detection%20and))）。总体来看，**结合大模型评估、小模型判别、知识检索和不确定度估计**的混合策略是近期突破的趋势。这些方法在诸如真实场景对话、医学问答等高风险应用中进行测试，显著提高了发现模型谬误的能力。例如有报告称 Meta AI 的一个幻觉检测模型在测试中能识别出92%的不正确输出（相对于人工标注) ([The Beginner's Guide to Hallucinations in Large Language Models](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models#:~:text=Models%20www,inconsistent%20with%20the%20input%20data))。今后研究还将在**实时检测**（如对话时即时标记不可信句子）和**跨语言幻觉**（不同语言模型的幻觉行为）等方面深入推进 ([[PDF] 2024: Can LLMs detect their Hallucinations - ACL Anthology](https://aclanthology.org/2024.osact-1.17.pdf#:~:text=,datasets%20in%20the%20Arabic%20language)) ([[PDF] 2024: Can LLMs detect their Hallucinations - ACL Anthology](https://aclanthology.org/2024.osact-1.17.pdf#:~:text=Abstract,datasets%20in%20the%20Arabic%20language))。

# 2022 年以来的相关论文精选

以下列出近年来有关模型幻觉和事实性检测的重要研究，包括综述论文、具体方法论文以及 TruthfulQA 官方论文，并给出其核心结论和链接：

- **Lin et al., ACL 2022** – _“TruthfulQA: Measuring How Models Mimic Human Falsehoods”_ ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=We%20propose%20a%20benchmark%20to,The%20largest%20models%20were)) ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=misconception,tuning%20using))：提出 TruthfulQA 基准数据集（817问，38类）用于评估语言模型回答的真实性。研究发现**现有大型模型在真事实问答上表现不佳，且模型规模越大反而错误率越高**：最佳模型真实回答率仅58%，远低于人类的94% ([TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology](https://aclanthology.org/2022.acl-long.229/#:~:text=misconception,tuning%20using))。作者建议通过特殊微调而非单纯扩大模型来提升真实性。该论文是 TruthfulQA 数据集的官方发表。
    
- **Ji et al., ACM CS 2022** – _“Survey of Hallucination in NLG”_ ([[2202.03629] Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629#:~:text=texts%2C%20but%20these%20have%20never,serves%20to%20facilitate%20collaborative%20efforts))：全面综述了**自然语言生成中的幻觉问题**。涵盖了幻觉产生的原因、评价指标、缓解方法以及各下游任务（摘要、对话、问答、数据描述、翻译、多模态等）中幻觉研究进展 ([[2202.03629] Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629#:~:text=texts%2C%20but%20these%20have%20never,serves%20to%20facilitate%20collaborative%20efforts))。此综述将已有工作分为**通用方法**和**任务特定**两部分，梳理了大量文献，指出幻觉现象普遍存在且尚未有统一解决方案 ([[2202.03629] Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629#:~:text=prone%20to%20hallucinate%20unintended%20text%2C,tasks%2C%20namely%20abstractive%20summarization%2C%20dialogue)) ([[2202.03629] Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629#:~:text=texts%2C%20but%20these%20have%20never,serves%20to%20facilitate%20collaborative%20efforts))。为研究者提供了系统的知识框架和未来方向展望。
    
- **Manakul et al., EMNLP 2023** – _“SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection”_ ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=in%20a%20zero,our%20approach%20to%20several%20baselines)) ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=SelfCheckGPT%20can%3A%20i%29%20detect%20non,level))：提出了一种无需外部知识库、无需模型内部信息的**黑盒幻觉检测**方法。通过让模型对同一输入生成多次回答并评估一致性来判断真假 ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=in%20a%20zero,our%20approach%20to%20several%20baselines))。实验表明该方法在句子级检测的精确率-召回曲线下表现优于多种基线，并能有效为整段生成排序真实性 ([SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models - ACL Anthology](https://aclanthology.org/2023.emnlp-main.557/#:~:text=SelfCheckGPT%20can%3A%20i%29%20detect%20non,level))。这是首批专为**API接入的大型模型**设计的事实一致性检测方法之一，展示了模型自我不一致性作为判据的潜力。
    
- **Chen et al., CIKM 2023** – _“Hallucination Detection: Robustly Discerning Reliable Answers in LLMs”_ ([[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org/abs/2407.04121#:~:text=bilingual%20question,mitigating%20hallucination%20in%20the%20future)) ([[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org/abs/2407.04121#:~:text=LLMs%20and%20a%20comprehensive%20set,hallucination%20in%20the%20future%20work))：介绍了**RelD 判别模型**及其构建的 RelQA 数据集。RelD 在大量 LLM 生成的问答数据上训练，对不同模型、不同领域的回答进行真伪分类 ([[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org/abs/2407.04121#:~:text=we%20propose%20a%20robust%20discriminator,a%20thorough%20analysis%20of%20the))。结果显示其对内/外分布的数据都能**准确检测幻觉答案**，提升了跨模型的一致可靠性 ([[2407.04121] Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://arxiv.org/abs/2407.04121#:~:text=LLMs%20and%20a%20comprehensive%20set,hallucination%20in%20the%20future%20work))。该研究还分析了幻觉的类型和成因，为日后改进提供了见解。体现了通过**监督学习专门判别器**来应对幻觉的有效性。
    
- **Huang et al., TOIS 2024** – _“A Survey on Hallucination in LLMs: Principles, Taxonomy, Challenges”_ ([A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/html/2311.05232v2#:~:text=hallucinations,knowledge%20boundaries%20in%20LLM%20hallucinations))：最新的针对大型语言模型的幻觉综述。提出了**LLM 幻觉的分类方法**，深入分析了大模型产生幻觉的原因 ([A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/html/2311.05232v2#:~:text=hallucinations,language%20models%20and))。全面总结了现有**幻觉检测方法和基准**，以及**缓解策略**（从数据、训练到推理层面）和检索增强模型的局限 ([A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/html/2311.05232v2#:~:text=hallucinations,knowledge%20boundaries%20in%20LLM%20hallucinations))。最后讨论了未来方向，包括多模态幻觉和知识边界等开放问题 ([A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/html/2311.05232v2#:~:text=representative%20methodologies%20for%20mitigating%20LLM,knowledge%20boundaries%20in%20LLM%20hallucinations))。该综述突出强调了LLM幻觉相较传统NLG的新挑战，也是对近两年迅速发展的相关研究的系统总结。