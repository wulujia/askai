+++
date = '2025-04-17T10:35:20+08:00'
draft = false
title = 'AI æ¨¡å‹å¾®è°ƒ'
+++

> ä½œä¸ºä¸€å®¶å°ç§‘æŠ€å…¬å¸ï¼Œæƒ³è¦å¾®è°ƒè‡ªå·±çš„å° AI æ¨¡å‹ï¼Œç”¨äºè§£å†³ç‰¹å®šå‚ç›´é¢†åŸŸçš„é—®é¢˜ã€‚è¿™ç±» AI æ¨¡å‹å¾®è°ƒï¼Œå…·ä½“éœ€è¦ä½¿ç”¨åˆ°å“ªäº›æŠ€æœ¯ï¼Ÿå“ªäº›è®¾å¤‡ï¼Ÿå“ªäº›å·¥å…·ï¼Ÿæœ‰å“ªäº›æ–‡ç« æˆ–è€…è®ºæ–‡å¯¹æ­¤æœ‰è¯¦ç»†çš„å…¥é—¨è¯´æ˜ï¼Ÿè¯·å¸®æˆ‘åšä¸€ä¸ªä»æ–°æ‰‹å…¥é—¨åˆ°æˆä¸ºè¡Œä¸šä¸“å®¶çš„å­¦ä¹ æ¸…å•ã€‚
>

ä¸‹é¢è¿™ä»½å­¦ä¹ æ¸…å•ï¼Œæ—¨åœ¨å¸®åŠ©ä¸€å®¶å°ç§‘æŠ€å…¬å¸é€æ­¥æŒæ¡åœ¨ç‰¹å®šå‚ç›´é¢†åŸŸå¾®è°ƒâ€œå°â€AIæ¨¡å‹çš„å…³é”®æŠ€æœ¯å’Œæ–¹æ³•ã€‚æ¸…å•ä»æ–°æ‰‹å…¥é—¨åˆ°èµ„æ·±ä¸“å®¶ï¼Œåˆ†é˜¶æ®µæ¶µç›–æ‰€éœ€çš„ç†è®ºçŸ¥è¯†ã€å·¥å…·é“¾ã€ç¡¬ä»¶è®¾å¤‡å’Œå…¸å‹çš„å‚è€ƒæ–‡çŒ®/æ–‡ç« /è®ºæ–‡ã€‚å¸Œæœ›èƒ½å¸®åŠ©ä½ ä»¬æ›´ç³»ç»Ÿåœ°è§„åˆ’å­¦ä¹ å’Œå®è·µè·¯çº¿ã€‚

---

## ä¸€ã€åŸºç¡€é˜¶æ®µï¼šç†è§£æ ¸å¿ƒæ¦‚å¿µä¸ç†è®º

### 1.1 æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ åŸºç¡€
- **ç›®æ ‡**ï¼šæŒæ¡ç¥ç»ç½‘ç»œã€æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µä¸å¸¸ç”¨æŠ€å·§ï¼Œä¸ºåç»­çš„æ¨¡å‹å¾®è°ƒæ‰“å¥½åŸºç¡€ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼šçº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ç­‰  
  - æ·±åº¦å­¦ä¹ åŸºç¡€ï¼šå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–æ–¹æ³•ï¼ˆSGDã€Adam ç­‰ï¼‰  
- **æ¨èèµ„æº**ï¼š  
  - ã€ŠDeep Learningã€‹ (Ian Goodfellow, Yoshua Bengio, Aaron Courville)  
  - Coursera/å´æ©è¾¾æœºå™¨å­¦ä¹ è¯¾ç¨‹  
  - Fast.ai æ·±åº¦å­¦ä¹ å…è´¹è¯¾ç¨‹  

### 1.2 è¿ç§»å­¦ä¹ ä¸å¾®è°ƒ (Fine-tuning)
- **ç›®æ ‡**ï¼šç†è§£ä¸ºä½•è¦åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸Šè¿›è¡Œå¾®è°ƒï¼Œäº†è§£å¾®è°ƒçš„å¸¸ç”¨æ–¹æ³•ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - è¿ç§»å­¦ä¹ çš„æ¦‚å¿µï¼šä»å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹è¿ç§»åˆ°å°è§„æ¨¡ç›®æ ‡ä»»åŠ¡  
  - å¾®è°ƒæ–¹æ³•ï¼šå…¨æ¨¡å‹å¾®è°ƒ(Fine-tune Everything)ã€å†»ç»“éƒ¨åˆ†å±‚(Freeze)ã€å¢é‡è®­ç»ƒç­‰  
  - å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³• (PEFT)ï¼šå¦‚ LoRAã€P-tuningã€Adapterã€Prefix Tuning ç­‰  
- **æ¨èèµ„æº**ï¼š  
  - [Hugging Face è¿ç§»å­¦ä¹ å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/training)  
  - **è®ºæ–‡**ï¼š  
    - [Howard & Ruder (2018) - â€œUniversal Language Model Fine-tuning for Text Classification (ULMFiT)â€](https://arxiv.org/abs/1801.06146)  
    - [Houlsby et al. (2019) - â€œParameter-Efficient Transfer Learning for NLPâ€](https://arxiv.org/abs/1902.00751)  

### 1.3 å¸¸è§é¢„è®­ç»ƒæ¨¡å‹ä¸æ¶æ„
- **ç›®æ ‡**ï¼šç†Ÿæ‚‰å¸‚é¢ä¸Šä¸»æµçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œäº†è§£å®ƒä»¬çš„ç‰¹ç‚¹ä¸é€‚ç”¨åœºæ™¯ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - BERTã€RoBERTaã€ALBERT ç­‰ç¼–ç æ¨¡å‹  
  - GPTã€GPT-2ã€GPT-3 ç­‰ç”Ÿæˆæ¨¡å‹  
  - T5ã€BARTã€GPT-Neoã€BLOOM ç­‰å¼€æº/å¤§è§„æ¨¡ Transformer  
- **æ¨èèµ„æº**ï¼š  
  - **è®ºæ–‡**ï¼š  
    - [Devlin et al. (2018) - â€œBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingâ€](https://arxiv.org/abs/1810.04805)  
    - [Radford et al. (2019) - â€œLanguage Models are Unsupervised Multitask Learners (GPT-2)â€](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
    - [Brown et al. (2020) - â€œLanguage Models are Few-Shot Learners (GPT-3)â€](https://arxiv.org/abs/2005.14165)  

---

## äºŒã€è¿›é˜¶é˜¶æ®µï¼šå¾®è°ƒæŠ€æœ¯ä¸å®æˆ˜

### 2.1 å…¨é‡å¾®è°ƒä¸è½»é‡åŒ–å¾®è°ƒ
- **ç›®æ ‡**ï¼šå¯¹æ¯”å…¨é‡å¾®è°ƒ(æ›´æ–°æ‰€æœ‰æ¨¡å‹å‚æ•°)ä¸å„ç§è½»é‡åŒ–å¾®è°ƒ(ä»…æ›´æ–°éƒ¨åˆ†å‚æ•°æˆ–æ·»åŠ å°è§„æ¨¡å‚æ•°)ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - å…¨é‡å¾®è°ƒ (Fine-tune Everything) çš„ä¼˜ç¼ºç‚¹ï¼šéœ€è¦æ›´å¤šç®—åŠ›ä¸æ•°æ®ï¼›çµæ´»åº¦é«˜  
  - è½»é‡åŒ–å¾®è°ƒ (PEFT)ï¼š  
    - **LoRA** (Low-Rank Adaptation)ï¼šåªå­¦ä¹ ä½ç§©çŸ©é˜µçš„æ›´æ–°ï¼Œæ˜¾è‘—å‡å°‘å‚æ•°é‡  
    - **Adapter**ï¼šåœ¨ Transformer å±‚æ’å…¥å°å‹ Adapter æ¨¡å—  
    - **Prefix Tuning**ï¼šåœ¨è¾“å…¥åºåˆ—ä¸­å¢åŠ å¯å­¦ä¹ çš„å‰ç¼€å‘é‡  
    - **P-tuning / Prompt Tuning**ï¼šå¯¹æ¨¡å‹è¾“å…¥çš„ embedding å±‚è¿›è¡Œå¯å­¦ä¹ çš„ prompts  
- **æ¨èèµ„æº**ï¼š  
  - [Hugging Face PEFT åº“](https://github.com/huggingface/peft)  
  - **è®ºæ–‡**ï¼š  
    - [Hu et al. (2021) - â€œLoRA: Low-Rank Adaptation of Large Language Modelsâ€](https://arxiv.org/abs/2106.09685)  
    - [Lester et al. (2021) - â€œThe Power of Scale for Parameter-Efficient Prompt Tuningâ€](https://arxiv.org/abs/2104.08691)  

### 2.2 çŸ¥è¯†è’¸é¦ä¸æ¨¡å‹å‹ç¼©
- **ç›®æ ‡**ï¼šåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå­¦ä¹ å¦‚ä½•å°†å¤§æ¨¡å‹çš„çŸ¥è¯†è’¸é¦åˆ°å°æ¨¡å‹ï¼Œæˆ–è¿›ä¸€æ­¥å¯¹æ¨¡å‹è¿›è¡Œå‰ªæã€é‡åŒ–ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - çŸ¥è¯†è’¸é¦ (Knowledge Distillation)ï¼šæ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œæµç¨‹  
  - æ¨¡å‹å‰ªæ (Pruning)ï¼šä¿®å‰ªç¥ç»ç½‘ç»œä¸­ä¸é‡è¦çš„æƒé‡  
  - æ¨¡å‹é‡åŒ– (Quantization)ï¼šåˆ©ç”¨å®šç‚¹æˆ–ä½æ¯”ç‰¹æ¥è¡¨ç¤ºæƒé‡  
- **æ¨èèµ„æº**ï¼š  
  - **è®ºæ–‡**ï¼š  
    - [Hinton et al. (2015) - â€œDistilling the Knowledge in a Neural Networkâ€](https://arxiv.org/abs/1503.02531)  
    - [Han et al. (2015) - â€œDeep Compression: Compressing Deep Neural Networksâ€¦â€](https://arxiv.org/abs/1510.00149)  

### 2.3 æ•°æ®å‡†å¤‡ä¸æ ‡æ³¨
- **ç›®æ ‡**ï¼šç¡®ä¿å¾®è°ƒæ‰€éœ€çš„æ•°æ®é«˜è´¨é‡ä¸”é€‚åˆç›®æ ‡é¢†åŸŸï¼›æŒæ¡å¿…è¦çš„æ•°æ®å¢å¼ºå’Œæ ‡ç­¾æ ‡æ³¨æ–¹æ³•ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - æ•°æ®æ¸…æ´—ã€å»é‡ã€é™å™ª  
  - é¢†åŸŸæ•°æ®æ”¶é›†ï¼šç‰¹å®šè¡Œä¸šæ–‡æ¡£ã€APIæ–‡æ¡£ã€æ—¥å¿—æ•°æ®ã€å®¢æˆ·é—®ç­”ç­‰  
  - åŠè‡ªåŠ¨æˆ–äººå·¥æ ‡æ³¨å·¥ä½œæµç¨‹  
  - æ•°æ®å¢å¼º(ä½¿ç”¨åŒä¹‰æ›¿æ¢ã€GPT-based æ•°æ®ç”Ÿæˆç­‰)  
- **å®è·µå»ºè®®**ï¼š  
  - ä½¿ç”¨ [Label Studio](https://github.com/heartexlabs/label-studio) ç­‰å¼€æºæ ‡æ³¨å¹³å°  
  - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆ– Python è„šæœ¬åšæ‰¹é‡æ•°æ®æ¸…æ´—  
  - ç†Ÿæ‚‰ [spaCy](https://spacy.io/) æˆ– [NLTK](https://www.nltk.org/) åšæ–‡æœ¬å¤„ç†  

### 2.4 å®æˆ˜ï¼šä½¿ç”¨å¼€æºå·¥å…·é“¾ (ä»¥ Hugging Face ä¸ºä¾‹)
- **ç›®æ ‡**ï¼šæŒæ¡ä»æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚  
- **æ­¥éª¤**ï¼š  
  1. **é€‰æ‹©æ¨¡å‹**ï¼šåœ¨ [Hugging Face Model Hub](https://huggingface.co/models) ä¸Šé€‰å–é€‚åˆè‡ªå·±ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ï¼ˆè¯­è¨€ã€è§„æ¨¡ã€å¼€æºè®¸å¯ï¼‰ã€‚  
  2. **æ•°æ®å‡†å¤‡**ï¼šæ ¹æ®ä»»åŠ¡æ ¼å¼(JSON, CSV, TSV)åŠ è½½ï¼Œç¼–å†™ Dataset ç±»æˆ–ä½¿ç”¨ `datasets` åº“ã€‚  
  3. **å¾®è°ƒ**ï¼šä½¿ç”¨ `transformers` åº“çš„ Trainer æˆ–è‡ªå®šä¹‰è®­ç»ƒè„šæœ¬ï¼›é€‰æ‹©åˆé€‚çš„å¾®è°ƒæ–¹å¼ï¼ˆå…¨é‡æˆ–è½»é‡åŒ–ï¼‰ã€‚  
  4. **éªŒè¯ä¸è¯„ä¼°**ï¼šæŒ‡æ ‡(å‡†ç¡®ç‡ã€F1ã€BLEUã€ROUGE ç­‰)ï¼Œæ³¨æ„è¿‡æ‹Ÿåˆç°è±¡ã€‚  
  5. **éƒ¨ç½²**ï¼šä½¿ç”¨ `torch.save` æˆ– `transformers` pipeline / FastAPI / Docker ç­‰è¿›è¡Œä¸Šçº¿ã€‚  
- **æ¨èèµ„æº**ï¼š  
  - Hugging Face å®˜æ–¹æ•™ç¨‹ [Hugging Face Course](https://huggingface.co/course)  
  - Hugging Face YouTube é¢‘é“  

---

## ä¸‰ã€é«˜é˜¶é˜¶æ®µï¼šæ·±å…¥ç†è§£ä¸ä¼˜åŒ–æ¨¡å‹

### 3.1 é«˜çº§ä¼˜åŒ–æŠ€å·§
- **ç›®æ ‡**ï¼šåœ¨ç®—åŠ›æœ‰é™æˆ–å¯¹æ—¶å»¶æ•æ„Ÿçš„åœºæ™¯ä¸‹ï¼Œå­¦ä¼šè°ƒä¼˜è¶…å‚æ•°å¹¶åŠ é€Ÿæ¨ç†ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - è¶…å‚æ•°æœç´¢ï¼šå­¦ä¹ ç‡ã€batch sizeã€æ¢¯åº¦ç´¯ç§¯ã€æƒé‡è¡°å‡ç­‰  
  - æ··åˆç²¾åº¦è®­ç»ƒ (FP16 / BF16)  
  - åŠ¨æ€å›¾ä¸é™æ€å›¾ä¼˜åŒ–ï¼ˆPyTorch vs TensorFlowï¼‰  
  - åˆ†å¸ƒå¼è®­ç»ƒï¼šData Parallel / Model Parallel / Pipeline Parallel  
- **æ¨èèµ„æº**ï¼š  
  - NVIDIA Developer åšå®¢ï¼ˆ[Mixed Precision Training](https://developer.nvidia.com/mixed-precision-training)ï¼‰  
  - [PyTorch Lightning](https://github.com/Lightning-AI/lightning) æä¾›çš„åˆ†å¸ƒå¼ / æ··åˆç²¾åº¦å·¥å…·  

### 3.2 ç‰¹æ®Šé¢†åŸŸä¸å¤šæ¨¡æ€å¾®è°ƒ
- **ç›®æ ‡**ï¼šæ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ï¼Œæ¯”å¦‚å›¾æ–‡ã€è¯­éŸ³ã€éŸ³è§†é¢‘ç­‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - Vision Transformer (ViT) å¾®è°ƒ  
  - CLIP (Contrastive Language-Image Pre-training)  
  - å¯ç”¨äºå¤šæ¨¡æ€çš„å¾®è°ƒæŠ€å·§ï¼šPrompt Tuning + å¤šæ¨¡æ€èåˆ  
- **æ¨èèµ„æº**ï¼š  
  - [Radford et al. (2021) - â€œLearning Transferable Visual Models From Natural Language Supervision (CLIP)â€](https://arxiv.org/abs/2103.00020)  
  - OpenAI Whisper ç›¸å…³é¡¹ç›®ï¼ˆè¯­éŸ³è½¬æ–‡æœ¬ï¼‰  

### 3.3 é’ˆå¯¹ç”Ÿæˆå¼å¤§æ¨¡å‹çš„å¾®è°ƒ (Instruction Tuning / RLHF)
- **ç›®æ ‡**ï¼šæŒæ¡å¤§è¯­è¨€æ¨¡å‹(å¦‚ GPT ç³»åˆ—)çš„æŒ‡ä»¤å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹ã€‚  
- **å­¦ä¹ å†…å®¹**ï¼š  
  - Instruction Tuningï¼šæ”¶é›†æŒ‡ä»¤-å›ç­”æ•°æ®é›†ï¼Œå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒæŒ‡ä»¤åŒ–å¾®è°ƒ  
  - RLHF (Reinforcement Learning from Human Feedback)ï¼šé€šè¿‡äººç±»åé¦ˆçš„å¥–åŠ±ä¿¡å·æ¥è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼ˆä»¥ ChatGPT ä¸ºä¾‹ï¼‰  
  - å®‰å…¨æ€§ã€é“å¾·è§„èŒƒã€åè§ä¸å®¡è®¡  
- **æ¨èèµ„æº**ï¼š  
  - **è®ºæ–‡**ï¼š  
    - [Ouyang et al. (2022) - â€œTraining language models to follow instructions with human feedbackâ€ (InstructGPT)](https://arxiv.org/abs/2203.02155)  
  - Deep RL ç›¸å…³è¯¾ç¨‹ (å¦‚ OpenAI Spinning Up)  

---

## å››ã€è®¾å¤‡ä¸ç¡¬ä»¶ç¯å¢ƒ

1. **GPU æœåŠ¡å™¨ / äº‘å¹³å°**  
   - å…¥é—¨å¯ä½¿ç”¨å•å¡å¦‚ NVIDIA RTX 3090 / 4090 (24GB æ˜¾å­˜)  
   - å¦‚æœéœ€è¦åˆ†å¸ƒå¼è®­ç»ƒæˆ–æ›´å¤§æ¨¡å‹ï¼Œå¯ä½¿ç”¨å¤šå¡ A100 / V100 / T4 / H100  
   - äº‘ç«¯å¹³å°ï¼šAWSã€GCPã€Azureã€å›½å†…å¦‚é˜¿é‡Œäº‘ã€è…¾è®¯äº‘çš„ GPU å®ä¾‹  

2. **CPU ç¯å¢ƒ**  
   - é€‚ç”¨äºå°è§„æ¨¡æ¨ç†ã€æˆ–é€šè¿‡è’¸é¦å’Œé‡åŒ–åçš„å°æ¨¡å‹éƒ¨ç½²  

3. **æœ¬åœ°å¼€å‘ç¯å¢ƒ**  
   - æ¨èä½¿ç”¨ Anaconda ç®¡ç† Python ç¯å¢ƒ  
   - å¿…è¦çš„åº“ï¼šPyTorch / TensorFlowã€transformersã€datasetsã€peftã€Lightning ç­‰  

---

## äº”ã€æŒç»­è·Ÿè¿›ä¸è¡Œä¸šå‰æ²¿

### 5.1 ç¤¾åŒºä¸ä¼šè®®
- **ç›®æ ‡**ï¼šä¸ä¸šå†…ä¸“å®¶ã€å¼€å‘è€…ç¤¾ç¾¤ä¿æŒè”ç³»ï¼Œéšæ—¶è·Ÿè¿›æ–°è¿›å±•ã€‚  
- **å»ºè®®åŠ å…¥çš„ç¤¾åŒº**ï¼š  
  - Hugging Face è®ºå›  
  - PyTorch è®ºå› / GitHub Issues  
  - Reddit / Stack Overflow ç­‰æŠ€æœ¯é—®ç­”ç¤¾åŒº  
- **é‡è¦ä¼šè®®**ï¼š  
  - NeurIPS, ICML, ICLR, ACL, EMNLP ç­‰  

### 5.2 é‡è¦å‰æ²¿è®ºæ–‡ / é¢„å°æœ¬
- **å¸¸çœ‹å¹³å°**ï¼š  
  - [arXiv](https://arxiv.org/)  
  - [Papers With Code](https://paperswithcode.com/)  
- **å…³æ³¨æ–¹å‘**ï¼š  
  - æ›´é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæ–¹æ³• (PEFT) æ–°è¿›å±•  
  - æ–°å‹å¤§è¯­è¨€æ¨¡å‹(å¦‚ LLaMA åŠå…¶è¡ç”Ÿ)çš„å¼€æºä¸ç¤¾åŒºå®è·µ  
  - æ›´å®‰å…¨ã€æ›´å¯æ§çš„ç”Ÿæˆå¼æ¨¡å‹  

### 5.3 å®è·µç»éªŒä¸è¡Œä¸šæ¡ˆä¾‹
- **ç›®æ ‡**ï¼šä»å®é™…æ¡ˆä¾‹ä¸­å­¦ä¹ å¾®è°ƒæŠ€æœ¯åœ¨ä¸åŒé¢†åŸŸçš„è½åœ°æ–¹å¼ã€‚  
- **æ–¹å¼**ï¼š  
  - å…³æ³¨ä¸€äº›å¼€æºé¡¹ç›®çš„å¾®è°ƒæ¡ˆä¾‹ï¼Œå¦‚ [OpenChatKit](https://github.com/togethercomputer/OpenChatKit)ã€[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) ç­‰  
  - è·Ÿè¿›ä¼ä¸šåšå®¢ (OpenAIã€Googleã€Meta AI ç­‰) çš„å®˜æ–¹å‘å¸ƒæ¡ˆä¾‹  
  - æ’°å†™å’Œå¼€æºè‡ªå·±å…¬å¸çš„å®è·µç»éªŒï¼Œä¿ƒè¿›ä¸ç¤¾åŒºäº¤æµ  

---

## å…­ã€æ€»ç»“ä¸å­¦ä¹ è·¯å¾„å‚è€ƒ

1. **åˆçº§é˜¶æ®µ**ï¼š  
   - æŒæ¡æ·±åº¦å­¦ä¹ å’Œ Transformer åŸºç¡€  
   - äº†è§£å¾®è°ƒã€è½»é‡åŒ–å¾®è°ƒå’ŒçŸ¥è¯†è’¸é¦ç­‰æ–¹æ³•ç†è®º  
   - ç†Ÿæ‚‰ä¸€ä¸ªä¸»æµæ¡†æ¶ï¼ˆPyTorch æˆ– TensorFlowï¼‰ä¸ Hugging Face å·¥å…·é“¾  
2. **è¿›é˜¶é˜¶æ®µ**ï¼š  
   - æŒæ¡å„ç§å¾®è°ƒæ–¹æ³•ä¸å®æˆ˜æŠ€å·§  
   - ç†Ÿæ‚‰æ•°æ®æ¸…æ´—ã€æ ‡æ³¨ä¸è¯„ä¼°æŒ‡æ ‡  
   - å­¦ä¼šä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒä¸æ··åˆç²¾åº¦æ¥æå‡è®­ç»ƒæ•ˆç‡  
3. **é«˜é˜¶é˜¶æ®µ**ï¼š  
   - å…³æ³¨ç‰¹å®šåœºæ™¯ï¼ˆå¤šæ¨¡æ€ã€Instruction Tuningã€RLHF ç­‰ï¼‰  
   - æ·±å…¥ç†è§£æ¨¡å‹ä¼˜åŒ–ç­–ç•¥ï¼ˆè¶…å‚æœç´¢ã€è’¸é¦ã€é‡åŒ–ã€å‰ªæï¼‰  
   - æŒæ¡å®‰å…¨æ€§ã€åˆè§„ä¸åè§å®¡è®¡ç­‰é«˜çº§è¯é¢˜  
4. **ä¸“å®¶é˜¶æ®µ**ï¼š  
   - æŒç»­è·Ÿè¿›å‰æ²¿è®ºæ–‡ä¸æŠ€æœ¯ç¤¾åŒº  
   - å‚ä¸å¹¶å‘è¡¨åœ¨ä¸»æµä¼šè®®æˆ–ç¤¾åŒºå¼€æºé¡¹ç›®  
   - åœ¨å…·ä½“å‚ç›´é¢†åŸŸç§¯ç´¯æ·±åšçš„æ¡ˆä¾‹å®è·µç»éªŒ  

é€šè¿‡ä»¥ä¸Šè·¯çº¿ï¼Œä½ ä»¬å¯ä»¥ç³»ç»Ÿåœ°ä»åŸºç¡€ç†è®ºåˆ°å‰æ²¿å®è·µï¼Œæœ€ç»ˆåœ¨ç‰¹å®šå‚ç›´é¢†åŸŸå¾®è°ƒâ€œå°â€AIæ¨¡å‹å¹¶å°†å…¶æˆåŠŸåº”ç”¨åˆ°å®é™…ä¸šåŠ¡å½“ä¸­ã€‚ç¥ä½ ä»¬å­¦ä¹ é¡ºåˆ©ã€é¡¹ç›®æˆåŠŸè½åœ°ï¼

æ˜ç™½äº†ï¼Œæˆ‘å°†ä¸ºä½ æ•´ç†ä¸€ä»½ç³»ç»Ÿæ€§å­¦ä¹ æ¸…å•ï¼Œæ¶µç›–å¦‚ä½•ä»æ–°æ‰‹å…¥é—¨åˆ°æŒæ¡é¢å‘æ¨èç³»ç»Ÿã€SaaS ä¸äº’è”ç½‘ç›¸å…³åœºæ™¯çš„å°å‹ AI æ¨¡å‹å¾®è°ƒæ‰€éœ€çš„æ‰€æœ‰æ ¸å¿ƒçŸ¥è¯†ã€‚æ¸…å•å°†åŒ…æ‹¬æ¨¡å‹æ¶æ„çš„åŸºç¡€ç†è§£ã€å¾®è°ƒæŠ€æœ¯ã€æ‰€éœ€è®¾å¤‡ã€å¼€å‘å·¥å…·ï¼Œä»¥åŠä¸­è‹±æ–‡çš„ä¼˜è´¨å­¦ä¹ èµ„æºä¸è®ºæ–‡ã€‚
æˆ‘æ•´ç†å¥½åå°±ä¼šå‘Šè¯‰ä½ ã€‚

# å°å‹ AI æ¨¡å‹å¾®è°ƒå­¦ä¹ æ¸…å•

## 1. æ¨¡å‹æ¶æ„åŸºç¡€

å½“ä»Šä¸»æµçš„ **Transformer** æ¶æ„åœ¨ NLP é¢†åŸŸæ— å¤„ä¸åœ¨ã€‚Transformer é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶é«˜æ•ˆå»ºæ¨¡åºåˆ—ï¼Œè¢«å¹¿æ³›åº”ç”¨äºå„ç±»æ¨¡å‹ ([ Transformers](https://huggingface.co/docs/transformers/v4.39.0/index#:~:text=State,TensorFlow%2C%20and%20JAX))ã€‚åŸºäº Transformerï¼Œæˆ‘ä»¬è¡ç”Ÿå‡ºäº†ä¸åŒç±»å‹çš„æ¨¡å‹ï¼š

- **BERT**ï¼ˆ**Encoder-only** æ¨¡å‹ï¼‰: BERT ä½¿ç”¨Transformerç¼–ç å™¨åŒå‘å»ºæ¨¡å¥å­ï¼Œé¢„è®­ç»ƒä»»åŠ¡æ˜¯**é®ç›–è¯­è¨€æ¨¡å‹ï¼ˆMasked LMï¼‰**ï¼Œèƒ½åˆ©ç”¨ä¸Šä¸‹æ–‡é¢„æµ‹è¢«é®ç›–çš„è¯ ([How is the bidirectional context achieved in BERT?](https://ai.stackexchange.com/questions/46054/how-is-the-bidirectional-context-achieved-in-bert#:~:text=How%20is%20the%20bidirectional%20context,i%E2%88%921%20when%20predicting%20token%20i))ã€‚å› æ­¤ï¼ŒBERTæ“…é•¿è‡ªç„¶è¯­è¨€ç†è§£ç±»ä»»åŠ¡ï¼ˆåˆ†ç±»ã€é—®ç­”ç­‰ï¼‰ã€‚BERT-Baseæ¨¡å‹çº¦1.1äº¿å‚æ•°ï¼ŒBERT-Large 3.4äº¿å‚æ•°ï¼Œå¯åœ¨**å°è§„æ¨¡æ•°æ®**ä¸Šå…¨é‡å¾®è°ƒã€‚BERT åŠå…¶å˜ç§ï¼ˆå¦‚RoBERTaï¼‰åœ¨è®¸å¤šNLPä»»åŠ¡ä¸Šæ•ˆæœçªå‡ºã€‚

- **GPT ç³»åˆ—**ï¼ˆ**Decoder-only** è‡ªå›å½’æ¨¡å‹ï¼‰: GPTå®¶æ—ï¼ˆGPT-2ã€GPT-3 ç­‰ï¼‰ä½¿ç”¨Transformerè§£ç å™¨ï¼Œä»¥**è‡ªå›å½’**æ–¹å¼é¢„è®­ç»ƒï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰ ([How is the bidirectional context achieved in BERT?](https://ai.stackexchange.com/questions/46054/how-is-the-bidirectional-context-achieved-in-bert#:~:text=How%20is%20the%20bidirectional%20context,i%E2%88%921%20when%20predicting%20token%20i))ã€‚è¿™ç±»æ¨¡å‹æ“…é•¿ç”Ÿæˆç±»ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ã€‚GPTæ¨¡å‹å‚æ•°è§„æ¨¡ä»å°åˆ°å¤§ï¼ˆGPT-2ä¸º1.5äº¿è‡³15äº¿ä¸ç­‰ï¼ŒGPT-3è¾¾1750äº¿ï¼‰ã€‚å°å…¬å¸å¯ä½¿ç”¨å¼€æºçš„GPT-2/GPT-Neo/GPT-Jç­‰å°æ¨¡å‹åšå¾®è°ƒï¼Œç”¨äºå†…å®¹ç”Ÿæˆã€‚**æ³¨æ„**GPT-3ç­‰è¶…å¤§æ¨¡å‹ç”±äºé—­æºä¸”è§„æ¨¡åºå¤§ï¼Œä¸åœ¨å°å‹å¾®è°ƒèŒƒç•´ã€‚

- **LLaMA ç³»åˆ—**: **LLaMA**æ˜¯Metaåœ¨2023å¹´å‘å¸ƒçš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œæä¾›7Bã€13Bã€33Bã€65Bå‚æ•°ç‰ˆæœ¬ ([[2302.13971] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971#:~:text=,models%20to%20the%20research%20community))ã€‚å®ƒè¯æ˜äº†åªç”¨å…¬å¼€æ•°æ®ä¹Ÿèƒ½è®­ç»ƒå‡ºæ€§èƒ½ä¼˜ç§€çš„æ¨¡å‹ï¼šLLaMA-13Båœ¨å¤šæ•°åŸºå‡†ä¸Šè¶…è¶Šäº†GPT-3ï¼ˆ175Bï¼‰ï¼ŒLLaMA-65Bæ¥è¿‘å½“æ—¶æœ€å¥½çš„æ¨¡å‹ï¼ˆå¦‚Chinchilla-70Bï¼‰ ([[2302.13971] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971#:~:text=,models%20to%20the%20research%20community))ã€‚LLaMAé‡‡ç”¨ä¸GPTç±»ä¼¼çš„è§£ç æ¶æ„ï¼Œä½†åœ¨**æ¿€æ´»å‡½æ•°**ä¸Šç”¨SwiGLUæ›¿ä»£ReLUï¼Œå¹¶ä½¿ç”¨**æ—‹è½¬ä½ç½®åµŒå…¥**å–ä»£ç»å¯¹ä½ç½®ç¼–ç  ([Comparing the Performance of LLMs: A Deep Dive into RoBERTa, Llama-2, and Mistral-7b for Disaster Tweets Analysis with LoRa | by Mehdi Iraqi | Medium](https://medium.com/@mehdi.iraqui/comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-and-mistral-for-disaster-tweets-8069e717548a#:~:text=Llama%20model%20is%20an%20auto,of%20absolute%20learnable%20positional%20embeddings))ã€‚LLaMA 2 å¯¹æ¶æ„è¿›ä¸€æ­¥æ”¹è¿›ï¼šæ”¯æŒæœ€é•¿4096ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¼•å…¥**Grouped-Query Attention (GQA)**ä¼˜åŒ–é•¿åºåˆ—è§£ç æ•ˆç‡ ([Comparing the Performance of LLMs: A Deep Dive into RoBERTa, Llama-2, and Mistral-7b for Disaster Tweets Analysis with LoRa | by Mehdi Iraqi | Medium](https://medium.com/@mehdi.iraqui/comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-and-mistral-for-disaster-tweets-8069e717548a#:~:text=The%20recent%20released%20Llama%202,query%20attention%20%28GQA%29%20decoding))ã€‚LLaMAç³»åˆ—é€‚åˆå°å‹å…¬å¸å¾®è°ƒï¼Œå› ä¸º7B/13Bç­‰æ¨¡å‹è§„æ¨¡é€‚ä¸­ã€å¼€æºå¯æ§ï¼Œåœ¨ç»è¿‡æŒ‡ä»¤å¾®è°ƒåå¯ç”¨äºå¯¹è¯é—®ç­”ç­‰å‚ç›´åœºæ™¯ã€‚

- **Mistral 7B**: **Mistral**æ˜¯2023å¹´æ¨å‡ºçš„æ–°é”å¼€æºæ¨¡å‹ï¼Œå‚æ•°ä»…7.3äº¿å´åœ¨æ€§èƒ½ä¸Š**åª²ç¾æˆ–è¶…è¶Šæ›´å¤§çš„LLaMA-13B** ([Mistral 7B vs. Llama 2 - Tutorial - Lemonfox.ai](https://www.lemonfox.ai/tutorials/mistral-vs-llama#:~:text=Mistral%207B%20vs,to%20run%20on%20smaller%20hardware))ã€‚å®ƒåœ¨æ¶æ„ä¸Šèå…¥äº†åˆ›æ–°ï¼šé‡‡ç”¨**æ»‘åŠ¨çª—æ³¨æ„åŠ›**æœºåˆ¶ä»£æ›¿å®Œæ•´è‡ªæ³¨æ„åŠ›ï¼Œä½¿æ¯å±‚æ³¨æ„æœ€å¤šå…³æ³¨4096ä¸ªå‰åºtokenï¼Œä»è€Œå®ç°æ›´é•¿ä¸Šä¸‹æ–‡ä¸”è®¡ç®—æˆæœ¬çº¿æ€§æ‰©å±• ([Comparing the Performance of LLMs: A Deep Dive into RoBERTa, Llama-2, and Mistral-7b for Disaster Tweets Analysis with LoRa | by Mehdi Iraqi | Medium](https://medium.com/@mehdi.iraqui/comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-and-mistral-for-disaster-tweets-8069e717548a#:~:text=,decoded%20tokens%20in%20the%20sequence))ï¼›åŒæ—¶ç»“åˆäº†LLaMA-2çš„**GQA**æœºåˆ¶æå‡è§£ç å¹¶è¡Œæ•ˆç‡ ([Comparing the Performance of LLMs: A Deep Dive into RoBERTa, Llama-2, and Mistral-7b for Disaster Tweets Analysis with LoRa | by Mehdi Iraqi | Medium](https://medium.com/@mehdi.iraqui/comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-and-mistral-for-disaster-tweets-8069e717548a#:~:text=historical%20information%20beyond%20the%20window,decoded%20tokens%20in%20the%20sequence))ã€‚ç”±äºMistral-7Bæ€§èƒ½å‡ºè‰²ä¸”å¯¹ç¡¬ä»¶è¦æ±‚ä½ï¼ˆ7Bå¤§å°æ˜“äºéƒ¨ç½²ï¼‰ï¼Œéå¸¸é€‚åˆå°å›¢é˜Ÿé’ˆå¯¹ç‰¹å®šé¢†åŸŸåšå¾®è°ƒï¼Œåœ¨æ¨èç³»ç»Ÿã€èŠå¤©åŠ©æ‰‹ç­‰åº”ç”¨ä¸­å–å¾—æ¥è¿‘å¤§æ¨¡å‹çš„æ•ˆæœ ([Mistral 7B vs. Llama 2 - Tutorial - Lemonfox.ai](https://www.lemonfox.ai/tutorials/mistral-vs-llama#:~:text=Mistral%207B%20vs,to%20run%20on%20smaller%20hardware))ã€‚

*ç®€è€Œè¨€ä¹‹*ï¼šTransformeræ¶æ„æ˜¯åº•åº§ï¼Œ**BERT**ä»£è¡¨ç†è§£å‹çš„å°æ¨¡å‹ï¼Œ**GPT/LLaMA**ä»£è¡¨ç”Ÿæˆå‹çš„å¤§æ¨¡å‹æ–¹å‘ï¼Œè€Œ**LLaMAã€Mistral**ç­‰è¿‘æœŸå¼€æºçš„å°æ¨¡å‹åœ¨ä¿æŒè¾ƒå°å‚æ•°é‡çš„åŒæ—¶å±•ç°äº†å¼ºå¤§æ€§èƒ½ï¼Œé€‚åˆä½œä¸ºå°å‹å…¬å¸å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚

## 2. å¾®è°ƒæŠ€æœ¯è·¯çº¿

é’ˆå¯¹é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ‰å¤šç§å¾®è°ƒç­–ç•¥å¯é€‰ï¼Œä¸‹é¢æ¦‚è¿°ä»**å…¨é‡å¾®è°ƒ**åˆ°**å‚æ•°é«˜æ•ˆå¾®è°ƒ**å†åˆ°**è’¸é¦**çš„ä¸»è¦æŠ€æœ¯è·¯çº¿ï¼Œå¹¶æ¯”è¾ƒå…¶é€‚ç”¨æƒ…å¢ƒå’Œä¼˜åŠ£ï¼š

- **å…¨é‡å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰**ï¼šæœ€ä¼ ç»Ÿæ–¹æ³•ï¼Œè§£å†»æ¨¡å‹çš„**å…¨éƒ¨å‚æ•°**åœ¨ä¸‹æ¸¸æ•°æ®ä¸Šè®­ç»ƒã€‚é€‚ç”¨äºä¸‹æ¸¸æ•°æ®å……è¶³ä¸”æœ‰è¶³å¤Ÿç®—åŠ›æ”¯æ’‘çš„æƒ…å†µï¼Œå¯æœ€å¤§ç¨‹åº¦å‘æŒ¥æ¨¡å‹èƒ½åŠ›ã€‚**ä¼˜ç‚¹**ï¼šæ¨¡å‹å¯å®Œå…¨é€‚é…æ–°ä»»åŠ¡ï¼Œé€šå¸¸èƒ½è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚**ç¼ºç‚¹**ï¼šå¯¹å¤§æ¨¡å‹æ˜¾å­˜éœ€æ±‚æé«˜ï¼Œè®¡ç®—å¼€é”€å¤§ï¼›æ¯ä¸ªæ–°ä»»åŠ¡éƒ½è¦å‚¨å­˜ä¸€ä»½å®Œæ•´æ¨¡å‹å‰¯æœ¬ï¼Œä¸åˆ©äºå¤šä»»åŠ¡åœºæ™¯ã€‚æ¯”å¦‚å¾®è°ƒ175Bçš„GPT-3å‡ ä¹ä¸å¯è¡Œï¼Œéœ€è¦å·¨é‡GPUèµ„æº ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=reducing%20the%20number%20of%20trainable,trainable%20parameters%2C%20a%20higher%20training))ã€‚

- **LoRAï¼ˆä½ç§©é€‚é…ï¼‰** ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=reducing%20the%20number%20of%20trainable,trainable%20parameters%2C%20a%20higher%20training))ï¼šå…¸å‹çš„**å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰**æ–¹æ³•ä¹‹ä¸€ã€‚LoRAçš„æ€è·¯æ˜¯**å†»ç»“å¤§æ¨¡å‹åŸæœ‰æƒé‡ï¼Œä»…åœ¨éƒ¨åˆ†æƒé‡ä¸Šå åŠ å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µ** ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=which%20freezes%20the%20pre,We%20also))ã€‚è®­ç»ƒæ—¶ä»…æ›´æ–°è¿™äº›æ–°å¢çš„å°çŸ©é˜µï¼Œä»è€Œå¤§å¹…å‡å°‘è®­ç»ƒå‚æ•°å’Œæ˜¾å­˜å¼€é”€ã€‚ä¾‹å¦‚ï¼Œç›¸æ¯”ç›´æ¥å¾®è°ƒGPT-3 175Bï¼ŒLoRAå°†å¯è®­ç»ƒå‚æ•°å‡å°‘äº†**10000å€**ï¼Œæ˜¾å­˜éœ€æ±‚é™ä½3å€ï¼Œè€Œåœ¨RoBERTaã€GPT-2ç­‰æ¨¡å‹ä¸Šæ€§èƒ½ä¸å…¨é‡å¾®è°ƒç›¸å½“ ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=reducing%20the%20number%20of%20trainable,We%20also))ã€‚**ä¼˜ç‚¹**ï¼šå‚æ•°å¼€é”€æå°ï¼ˆé€šå¸¸åªéœ€åŸæ¨¡å‹0.1%ï½1%çš„å‚æ•°é‡ï¼‰ã€è®­ç»ƒé€Ÿåº¦å¿«ã€å¯åœ¨æ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒè¶…å¤§æ¨¡å‹ï¼Œå¹¶ä¸”å› ä¸ºåªå¢åŠ å°æ¨¡å—ï¼Œä¸å¼•å…¥æ¨ç†æ—¶çš„é¢å¤–å»¶è¿Ÿ ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=parameters%20by%2010%2C000%20times%20and,We%20also))ã€‚æ­¤å¤–ï¼Œä¸åŒä¸‹æ¸¸ä»»åŠ¡çš„LoRAæƒé‡å¯æŒ‰éœ€æ’æ‹”ï¼ŒåŒä¸€åŸºç¡€æ¨¡å‹å¯æ–¹ä¾¿åˆ‡æ¢ä»»åŠ¡ã€‚**ç¼ºç‚¹**ï¼šLoRAéœ€è¦å¯¹æ¨¡å‹æ¶æ„æœ‰æ‰€æ”¹åŠ¨ï¼ˆæ’å…¥ä½ç§©çŸ©é˜µï¼‰ï¼Œå®ç°ä¸Šè¾ƒå…¨é‡å¾®è°ƒå¤æ‚ä¸€ç‚¹ï¼›åœ¨æç«¯ç²¾è°ƒåœºæ™¯ä¸‹å¯èƒ½ç•¥é€Šäºå…¨é‡å¾®è°ƒçš„ä¸Šé™æ€§èƒ½ã€‚

- **Adapter æ¨¡å—**ï¼šå¦ä¸€ç±»å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œç”±Houlsbyç­‰äººæå‡ºã€‚å®ƒæ˜¯åœ¨Transformeræ¯å±‚åŠ å…¥å°å‹**é€‚é…å±‚**ï¼ˆå¦‚ä¸¤å±‚å…¨è¿æ¥ç“¶é¢ˆç½‘ç»œï¼‰ï¼Œä»…è®­ç»ƒè¿™äº›æ–°å¢å±‚å‚æ•°ã€‚é€‚ç”¨äºå¤šä»»åŠ¡åœºæ™¯ï¼Œæ¯ä¸ªä»»åŠ¡ä¸€ä¸ªadapterã€‚**ä¼˜ç‚¹**ï¼šåƒLoRAä¸€æ ·å¤§å¹…å‡å°‘è®­ç»ƒå‚æ•°ï¼Œä¸”æ¨¡å—åŒ–è®¾è®¡æ–¹ä¾¿ä¸åŒä»»åŠ¡é—´åˆ‡æ¢ã€‚**ç¼ºç‚¹**ï¼šå¼•å…¥é€‚é…å±‚ä¼šåœ¨æ¨ç†æ—¶å¢åŠ å°‘é‡è®¡ç®—å¼€é”€ï¼ˆç›¸æ¯”LoRAéœ€è¦é¢å¤–çš„å‰å‘è®¡ç®—ï¼‰ ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=parameters%20by%2010%2C000%20times%20and,We%20also))ï¼›åŒæ—¶Adapteréœ€è¦å¯¹æ¯å±‚ç½‘ç»œè¿›è¡Œä¿®æ”¹é›†æˆã€‚å®è·µä¸­LoRAå‡ºç°åï¼Œç”±äºæ›´ç®€å•é«˜æ•ˆï¼ŒAdapterä½¿ç”¨æ¸å°‘ï¼Œä½†BERTç­‰æ¨¡å‹ä¸ŠAdapterä»æ˜¯æœ‰æ•ˆæ–¹æ¡ˆã€‚

- **Prefix Tuning / Prompt Tuning**ï¼šè¿™ç±»æ–¹æ³•ä¸ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªä»»åŠ¡å­¦ä¹ ä¸€æ®µ**é¢å¤–çš„â€œå‰ç¼€æç¤ºâ€å‘é‡/åµŒå…¥**ï¼Œå¯çœ‹ä½œå¯è®­ç»ƒçš„æç¤ºtokenã€‚Prefix Tuningåœ¨æ¯å±‚è‡ªæ³¨æ„åŠ›åŠ å…¥å¯å­¦ä¹ çš„å‰ç¼€é”®å€¼å¯¹ï¼›è€ŒPrompt Tuningåˆ™åªåœ¨è¾“å…¥embeddingå±‚æ·»åŠ å¯å­¦ä¹ æç¤ºã€‚**é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦æå°å‚æ•°å¼€é”€ã€å¸Œæœ›ä¿æŒé¢„è®­ç»ƒæ¨¡å‹å®Œå…¨ä¸å˜çš„æƒ…å†µã€‚**ä¼˜ç‚¹**ï¼šæ–°å¢å‚æ•°é‡å¾®ä¹å…¶å¾®ï¼ˆæ¯”å¦‚GPT-2æ¯å±‚å‰ç¼€å‡ ç™¾ç»´å‘é‡ï¼‰ï¼Œæ˜“äºå…±äº«å’Œéƒ¨ç½²ï¼ˆåªéœ€åœ¨æ¨ç†æ—¶é™„åŠ æç¤ºï¼‰ã€‚**ç¼ºç‚¹**ï¼šå¾€å¾€éœ€è¦è¾ƒå¤§é‡çš„è®­ç»ƒæ­¥æ•°æ‰èƒ½å–å¾—ä¸å¾®è°ƒæ¥è¿‘çš„æ•ˆæœï¼Œå¯¹ä¸‹æ¸¸æ•°æ®å’Œè®­ç»ƒè¿­ä»£è¦æ±‚æ›´é«˜ï¼›å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œæ€§èƒ½å¯èƒ½ç•¥ä½äºLoRAç­‰æ–¹æ³• ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C%20a%20library%20of%20parameter,convenient%20to%20share%2C%20store%2C%20and))ã€‚é€šå¸¸é€‚åˆåœ¨æç¤ºå·¥ç¨‹åŸºç¡€ä¸Šåšå°‘é‡å­¦ä¹ çš„åœºæ™¯ã€‚

- **QLoRAï¼ˆé‡åŒ–+LoRAï¼‰**ï¼š2023å¹´æå‡ºçš„æ–°æ–¹æ³•ï¼Œå°†æ¨¡å‹æƒé‡**é‡åŒ–è‡³4ä½**è¡¨ç¤ºä»¥æå¤§é™ä½æ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶ç»“åˆLoRAè¿›è¡Œå¾®è°ƒ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,hours%20of%20finetuning%20on%20a))ã€‚å…¶æ ¸å¿ƒæ˜¯åœ¨ä¸å½±å“æ¨¡å‹æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå¯¹é¢„è®­ç»ƒæ¨¡å‹æƒé‡åš4-bité‡åŒ–å†»ç»“ï¼Œç„¶ååä¼ æ¢¯åº¦æ›´æ–°LoRAä½ç§©æƒé‡ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,hours%20of%20finetuning%20on%20a))ã€‚**é€‚ç”¨åœºæ™¯**ï¼šè¶…å¤§æ¨¡å‹ï¼ˆ30Bï½70B+ï¼‰çš„å¾®è°ƒï¼Œç¡¬ä»¶èµ„æºæœ‰é™æ—¶ã€‚**ä¼˜ç‚¹**ï¼šæ˜¾å­˜å ç”¨æä½ï¼Œä½¿å¾—**65Bå‚æ•°æ¨¡å‹å¯åœ¨å•å¼ 48GB GPUä¸Šå¾®è°ƒ**æˆä¸ºå¯èƒ½ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=match%20at%20L199%20enables%2033B,on%20a%20single%2046GB%20GPU))ï¼›å®éªŒè¯æ˜4-bitå¾®è°ƒæ€§èƒ½å‡ ä¹ä¸16-bitå…¨ç²¾åº¦å¾®è°ƒæŒå¹³ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,hours%20of%20finetuning%20on%20a))ã€‚ä¾‹å¦‚ï¼Œç ”ç©¶è€…ç”¨QLoRAåœ¨å•å¡24å°æ—¶å¾®è°ƒå‡ºGuanaco-33Bæ¨¡å‹ï¼Œå…¶åœ¨VicunaåŸºå‡†ä¸Šè¾¾åˆ°ChatGPTæ€§èƒ½çš„99% ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,innovations%20to%20save%20memory%20without))ã€‚**ç¼ºç‚¹**ï¼šå®ç°æµç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ç¨³å®šçš„4-bité‡åŒ–æ”¯æŒï¼ˆé€šå¸¸å€ŸåŠ©`bitsandbytes`åº“ï¼‰ï¼›æŸäº›æç«¯æƒ…å†µä¸‹4-bitè¿‘ä¼¼å¯èƒ½å¸¦æ¥å¾®å°ç²¾åº¦æŸå¤±ï¼Œä½†æ€»ä½“å½±å“å¾ˆå°ã€‚

- **çŸ¥è¯†è’¸é¦ï¼ˆDistillationï¼‰**ï¼šé€šè¿‡è®©å°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰å»æ¨¡ä»¿å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰çš„è¾“å‡ºåˆ†å¸ƒï¼Œå°†å¤§æ¨¡å‹çš„çŸ¥è¯†å‹ç¼©åˆ°å°æ¨¡å‹ä¸­ã€‚è’¸é¦é€šå¸¸åœ¨æ•™å¸ˆæ¨¡å‹å·²å¾®è°ƒå¥½çš„æƒ…å†µä¸‹è¿›è¡Œï¼Œä»¥æ— æ ‡ç­¾æ•°æ®æˆ–æ•™å¸ˆç”Ÿæˆçš„è½¯æ ‡ç­¾ä¸ºè®­ç»ƒç›‘ç£ã€‚**é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦**éƒ¨ç½²è½»é‡çº§æ¨¡å‹**ä»¥æ»¡è¶³å†…å­˜æˆ–æ—¶å»¶çº¦æŸçš„æƒ…å†µã€‚**ä¼˜ç‚¹**ï¼šå¯ä»¥è®­ç»ƒå‡ºè¿œå°äºæ•™å¸ˆä½†æ€§èƒ½æ¥è¿‘çš„æ¨¡å‹ã€‚ä¾‹å¦‚DistilBERTå°†BERTå‚æ•°å‡å°40%ã€æ¨ç†æé€Ÿ60%ï¼Œä»ä¿ç•™äº†åŸæ¨¡å‹çº¦97%çš„æ€§èƒ½ ([[PDF] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and ...](https://arxiv.org/pdf/1910.01108#:~:text=,of%20the%20language%20understanding))ã€‚å¯¹äºå°å‹å…¬å¸ï¼Œè¿™æ„å‘³ç€å¯ç”¨è¾ƒå°‘èµ„æºéƒ¨ç½²æ¥è¿‘å¤§æ¨¡å‹æ•ˆæœçš„æœåŠ¡ã€‚**ç¼ºç‚¹**ï¼šè’¸é¦éœ€è¦ç²¾å¿ƒè®¾è®¡æ•™å¸ˆè¾“å‡ºï¼ˆå¦‚æ¸©åº¦è°ƒèŠ‚ï¼‰ã€å……åˆ†çš„è®­ç»ƒæ•°æ®ï¼Œä»¥åŠå­¦ç”Ÿæ¨¡å‹è¶³å¤Ÿçš„å®¹é‡ï¼Œå¦åˆ™å¯èƒ½æ— æ³•å­¦åˆ°æ•™å¸ˆçš„æ·±å±‚çŸ¥è¯†ï¼›è’¸é¦è¿‡ç¨‹æœ¬èº«ä¹Ÿè¦è€—è´¹è®­ç»ƒèµ„æºã€‚è’¸é¦æ¨¡å‹çš„æ€§èƒ½ä¸Šé™å—å­¦ç”Ÿæ¨¡å‹ç»“æ„é™åˆ¶ï¼Œå¯èƒ½æ— æ³•å®Œå…¨å¤ç°æ•™å¸ˆæ€§èƒ½ã€‚

**æ€»ç»“**ï¼šå°å‹ç§‘æŠ€å…¬å¸å¯æ ¹æ®ä»»åŠ¡éœ€æ±‚ä¸èµ„æºçº¦æŸé€‰æ‹©å¾®è°ƒæ–¹æ¡ˆâ€”â€”æ•°æ®å……è£•ä¸”æ¨¡å‹ä¸å¤§æ—¶å¯ç›´æ¥å…¨é‡å¾®è°ƒï¼›æ¨¡å‹å¾ˆå¤§æˆ–å¤šä»»åŠ¡åœºæ™¯ä¸‹åº”é¦–é€‰LoRA/Adapterç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒ ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C%20a%20library%20of%20parameter,convenient%20to%20share%2C%20store%2C%20and))ï¼›èµ„æºæå…¶æœ‰é™åˆæƒ³å¾®è°ƒè¶…å¤§æ¨¡å‹åˆ™QLoRAæ˜¯åˆ©å™¨ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=match%20at%20L199%20enables%2033B,on%20a%20single%2046GB%20GPU))ï¼›è€Œä¸ºäº†éƒ¨ç½²æ•ˆç‡å¯ä»¥è€ƒè™‘å¯¹å¾®è°ƒåçš„æ¨¡å‹åšçŸ¥è¯†è’¸é¦ä»¥å¾—åˆ°å°æ¨¡å‹ã€‚é€šè¿‡ç»“åˆè¿™äº›æŠ€æœ¯æ‰‹æ®µï¼Œç”šè‡³å¯ä»¥åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šè®­ç»ƒå‡ºåª²ç¾ç™¾äº¿çº§æ¨¡å‹æ•ˆæœçš„ä¸“ç”¨æ¨¡å‹ã€‚

## 3. æ‰€éœ€ç¡¬ä»¶è®¾å¤‡

å¾®è°ƒæ‰€éœ€çš„ç¡¬ä»¶èµ„æºå–å†³äºæ¨¡å‹è§„æ¨¡å’Œå¾®è°ƒæ–¹æ³•ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ¨¡å‹å‚æ•°è§„æ¨¡è¶Šå¤§ï¼Œæ˜¾å­˜å’Œè®¡ç®—éœ€æ±‚è¶Šé«˜ã€‚ä¸‹é¢æŒ‰ç…§æ¨¡å‹å¤§è‡´è§„æ¨¡ï¼Œåˆ—å‡ºå…¸å‹æ‰€éœ€çš„GPUé…ç½®ï¼š

- **ç™¾å…†çº§å‚æ•°æ¨¡å‹ï¼ˆ<1äº¿ï¼‰**ï¼šå¦‚å°å‹Transformeræˆ–DistilBERTç­‰ï¼Œæ˜¾å­˜éœ€æ±‚ä½ï¼Œ**æ™®é€šç¬”è®°æœ¬GPUæˆ–Colabå…è´¹ç¯å¢ƒ**å³å¯å¾®è°ƒã€‚è¿™ç±»æ¨¡å‹å ç”¨æ˜¾å­˜é€šå¸¸åœ¨å‡ ç™¾MBåˆ°æ•°GBé‡çº§ï¼Œéå¸¸é€‚åˆå…¥é—¨å®éªŒã€‚

- **æ•°äº¿å‚æ•°çº§æ¨¡å‹ï¼ˆæ•°äº¿ï½10äº¿ï¼‰**ï¼šå¦‚BERT-baseï¼ˆ1.1äº¿ï¼‰ã€GPT-2å°æ¨¡å‹ï¼ˆ1.5äº¿ï¼‰ç­‰ã€‚**å•å¼ 12GB~16GBæ˜¾å­˜**è¶³ä»¥æ”¯æ’‘å…¨é‡å¾®è°ƒã€‚ä¾‹å¦‚åœ¨Tesla T4 (16GB)ä¸Šå¯å¾®è°ƒBERTç±»æ¨¡å‹ã€‚è®­ç»ƒæ‰€éœ€æ—¶é—´ä¹Ÿè¾ƒçŸ­ï¼Œå°å‹å…¬å¸å¯åœ¨ä¸€ä¸¤å¤©å†…å®Œæˆã€‚

- **ä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼ˆæ•°åäº¿å‚æ•°ï¼‰**ï¼šå¦‚GPT-2å¤§å‹(15äº¿)ã€GPT-J (6B)ã€LLaMA-7Bç­‰ã€‚**å•å¡24GB** GPUé€šå¸¸å¯å®¹çº³æ­¤çº§åˆ«æ¨¡å‹çš„å…¨ç²¾åº¦å¾®è°ƒï¼›è‹¥æ˜¾å­˜è¾ƒå°ï¼Œå¯å€ŸåŠ©8-bité‡åŒ–æˆ–Gradient Checkpointç­‰æŠ€å·§åœ¨16GBæ˜¾å­˜ä¸Šå°è¯•ã€‚é‡‡ç”¨LoRAç­‰æ–¹æ³•æ—¶ï¼Œåªéœ€åŠ è½½æ¨¡å‹æƒé‡ï¼ˆ7Bæ¨¡å‹FP16çº¦å 14GBæ˜¾å­˜ï¼‰ï¼Œè®­ç»ƒé¢å¤–å¼€é”€å¾ˆä½ï¼Œ**ä¸€å¼ 24GBå¡è¶³ä»¥LoRAå¾®è°ƒ7B-13Bæ¨¡å‹**ã€‚ä¾‹å¦‚æœ‰äººåœ¨RTX 3090 (24GB)ä¸Šå¯¹6B~7Bæ¨¡å‹æˆåŠŸæ‰§è¡ŒLoRAå¾®è°ƒã€‚ä½¿ç”¨QLoRAè¿›ä¸€æ­¥é™ä½æ˜¾å­˜åï¼Œ**13Bæ¨¡å‹åœ¨å•å¡16GBä¸Šä¹Ÿå¯å¾®è°ƒ**ï¼ˆ4-bité‡åŒ–ä½¿13Bæƒé‡ä»…çº¦6.5GBï¼‰ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=match%20at%20L199%20enables%2033B,on%20a%20single%2046GB%20GPU))ã€‚

- **å¤§è§„æ¨¡æ¨¡å‹ï¼ˆ20äº¿ï½70äº¿ï¼‰**ï¼šå¦‚LLaMA-13Bã€Bloom-7Bç­‰ã€‚è‹¥å…¨é‡å¾®è°ƒï¼Œè‡³å°‘éœ€è¦**2ï½4å¼ 24GB GPU**åˆ†å¸ƒå¼è®­ç»ƒï¼Œæˆ–ä½¿ç”¨å¦‚ **DeepSpeed ZeRO** ç­‰æŠ€æœ¯å°†ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦æ‹†åˆ†/Offloadä»¥é™ä½å•å¡å†…å­˜å ç”¨ã€‚LoRAå¾®è°ƒåˆ™å¤§å¹…é™ä½è¦æ±‚ï¼Œ13Bæ¨¡å‹åœ¨**å•å¼ 24GB** GPUä¸Šå³å¯é€šè¿‡LoRAè®­ç»ƒï¼ˆåªéœ€åŠ è½½æ¨¡å‹+å°‘é‡LoRAå‚æ•°ï¼‰ã€‚QLoRAåˆ™èƒ½å¤Ÿè®©**33Bæ¨¡å‹åœ¨24GBæ˜¾å­˜ä¸Šå¾®è°ƒ**ï¼Œ**65Bæ¨¡å‹åœ¨46GBæ˜¾å­˜ä¸Šå¾®è°ƒ** ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=match%20at%20L199%20enables%2033B,on%20a%20single%2046GB%20GPU)), è¿™å¯¹äºå°å…¬å¸è€Œè¨€æå¤§æ‹“å®½äº†å¯å¾®è°ƒçš„æ¨¡å‹èŒƒå›´ã€‚

- **è¶…å¤§è§„æ¨¡æ¨¡å‹ï¼ˆç™¾äº¿å‚æ•°ä»¥ä¸Šï¼‰**ï¼šå¦‚LLaMA-65Bã€GPT-3 175Bç­‰ã€‚å…¨é‡å¾®è°ƒå‡ ä¹åªèƒ½åœ¨å¤§å‹å¤šGPUé›†ç¾¤ä¸Šå®Œæˆï¼ˆæ•°åå¼ A100 80GBï¼‰ï¼Œå°å…¬å¸é€šå¸¸ä¸å…·å¤‡ã€‚å¹¸å¥½PEFTæŠ€æœ¯åœ¨æ­¤å¤§æ”¾å¼‚å½©ï¼š**QLoRAä½¿65Bæ¨¡å‹å•æœºå¯è®­** ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,hours%20of%20finetuning%20on%20a))ï¼Œä¸€äº›å¼€æºå·¥å…·ï¼ˆå¦‚Colossal-AIã€DeepSpeedï¼‰ä¹Ÿæ”¯æŒ8å¡A100 80GBä¸€é”®å¾®è°ƒæ•°ç™¾äº¿å‚æ•°æ¨¡å‹ã€‚ä½†è€ƒè™‘åˆ°æ•°æ®å’Œç®—åŠ›ä»£ä»·ï¼Œç™¾äº¿çº§æ¨¡å‹é€šå¸¸é€šè¿‡**è’¸é¦**è·å¾—å°æ¨¡å‹æ¥æ›¿ä»£ç›´æ¥å¾®è°ƒã€‚

> ğŸ’¡ *å®è·µæç¤º*: é’ˆå¯¹**æ¨èç³»ç»Ÿã€SaaSç­‰å‚ç›´åœºæ™¯**çš„è¯­è¨€æ¨¡å‹åº”ç”¨ï¼Œå¾€å¾€7B~13Bçš„å¾®è°ƒæ¨¡å‹å·²ç»è¶³å¤Ÿå®ç”¨ï¼Œå¯è¿è¡Œåœ¨å•æœºå•å¡ä¸Šï¼Œå»¶è¿Ÿå’Œæˆæœ¬ä¹Ÿå¯æ§ã€‚å› æ­¤å°å‹å›¢é˜Ÿåº”ä¼˜å…ˆè€ƒè™‘è¿™äº›è§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼Œé€šè¿‡åˆç†çš„å¾®è°ƒæŠ€æœ¯åœ¨æœ‰é™GPUä¸Šå–å¾—å‡ºè‰²æ•ˆæœã€‚

é™¤äº†è‡ªæœ‰ç¡¬ä»¶ï¼Œå°å‹å…¬å¸ä¹Ÿä¼šè€ƒè™‘**äº‘ç«¯ GPU**èµ„æºã€‚ä»¥ä¸‹æ˜¯å¸¸è§äº‘å¹³å°åŠå…¶GPUæ–¹æ¡ˆå¯¹æ¯”ï¼š

| å¹³å°        | å¯ç”¨GPUç±»å‹åŠè§„æ ¼                     | å®šä»·ï¼ˆæŒ‰éœ€å®ä¾‹ï¼‰ | ä¼˜åŠ¿                                                 | åŠ£åŠ¿                                   |
| ----------- | ------------------------------------ | ---------------- | ---------------------------------------------------- | -------------------------------------- |
| **AWS**     | NVIDIA A10G (24GB)ã€V100 (16GB)ã€A100 (40/80GB)ã€H100 ç­‰ | A10çº¦\$1.21/å°æ—¶ï¼ŒA100çº¦\$3.21/å° ([Top 15+ Cloud GPU ProvidersÂ For 2025](https://www.analyticsvidhya.com/blog/2023/12/top-gpus-you-must-explore/#:~:text=,third))ã€‘ | æœåŠ¡æˆç†Ÿç¨³å¥ï¼Œå…¨çƒåŒºåŸŸå¹¿æ³›ï¼›ç”Ÿæ€é›†æˆå®Œå–„ï¼ˆå¦‚SageMaker ([Top 15+ Cloud GPU ProvidersÂ For 2025](https://www.analyticsvidhya.com/blog/2023/12/top-gpus-you-must-explore/#:~:text=,SageMaker%29%2C%20global%20low))ã€‘ | ä»·æ ¼æ˜‚è´µï¼Œè®¡è´¹å¤æ‚ï¼›ç½‘ç»œå’Œå­˜å‚¨å¯èƒ½éœ€é¢å¤–è´¹ç”¨ |
| **Google GCP** | T4 (16GB)ã€A100 (40/80GB)ã€H100ï¼Œä»¥åŠè‡ªå®¶TPU | A100 80GBçº¦\$5/å°æ—¶ï¼ˆæŒ‰éœ€ï¼‰ï¼ŒPreemptibleå¯ä½è‡³1/3ä»· | æä¾›TPUç­‰ç‹¬æœ‰é€‰é¡¹ï¼Œé€‚åˆTensorFlowç­‰ï¼›ä¸è°·æ­Œäº‘æœåŠ¡è”åŠ¨æ–¹ä¾¿ | æŒ‰éœ€ä»·æ ¼é«˜äºAWSï¼ŒæŠ¢å å¼å®ä¾‹ä¸ç¨³å®šä¸­æ–­         |
| **Paperspace** | A4000 (16GB)ã€A5000/A6000 (24/48GB)ã€A100ç­‰ | å…¥é—¨GPU \$0.40/å°æ—¶èµ·ï¼Œé«˜ç«¯A100çº¦\$2.30/å° ([Paperspace vs Runpod: Discover The Best Cloud GPU Provider](https://www.poolcompute.com/compare/paperspace-vs-runpod#:~:text=%2A%20Paperspace%27s%20Flexible%20and%20User,learning%2C%20streamlining%20the%20setup%20process))ã€‘ | ä¸Šæ‰‹ç®€å•ï¼Œæä¾›Jupyterç¬”è®°æœ¬ç¯å¢ƒï¼›æœ‰ç¤¾åŒºå…è´¹é¢åº¦ï¼›é…ç½®çµæ´»æŒ‰å°æ—¶è®¡è´¹ | æ•°æ®ä¸­å¿ƒåŒºåŸŸç›¸å¯¹è¾ƒå°‘ï¼›ä¼ä¸šçº§é™„åŠ æœåŠ¡ä¸å¦‚äº‘å‚å•†ä¸°å¯Œ |
| **RunPod**  | GTX 1080(8GB)ç­‰æ—§å¡åˆ°æœ€æ–°A100/H100å‡æœ‰ | 1080çº¦\$0.20/å°æ—¶ï¼ŒA100 80GBç¤¾åŒºä»·\$1.69~\$1.99/å° ([[D] Cloud GPU Price Analysis - December 2024 - Reddit](https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/#:~:text=,Lambda%20A100%3A%20%241.29%2Fhr)) ([Paperspace vs Runpod: Discover The Best Cloud GPU Provider](https://www.poolcompute.com/compare/paperspace-vs-runpod#:~:text=Runpod%20provides%20competitive%20GPU%20pricing%2C,term%20commitments.%20Runpod))ã€‘ | ä»·æ ¼æå…·ç«äº‰åŠ›ï¼Œå¯æŒ‰åˆ†é’Ÿè®¡è´¹ï¼›ç¤¾åŒºæä¾›å¤šæ ·åŒ–ç®—åŠ›é€‰æ‹©ï¼Œå¼¹æ€§å¤§ | ç¤¾åŒºèŠ‚ç‚¹è´¨é‡å‚å·®ä¸é½ï¼ˆéœ€æŒ‘é€‰ä¿¡èª‰é«˜çš„èŠ‚ç‚¹ï¼‰ï¼›ç¼ºä¹å¤§å‹å‚å•†çš„ä¸€ç«™å¼æ”¯æŒ |
| **Lambda Labs** | A10 (24GB)ã€A6000 (48GB)ã€A100 (40/80GB)ã€H100 ç­‰ | A6000 48GB \$0.80/å°æ—¶ï¼ŒA100 40GB \$1.29/å° ([Lambda GPU Cloud | VM Pricing and Specs](https://lambda.ai/service/gpu-cloud/pricing#:~:text=On,50%20%2F%20GPU%20%2F%20hr))ã€‘ï¼ˆæŒ‰éœ€ï¼‰ | ä¸“æ³¨æ·±åº¦å­¦ä¹ ï¼Œç¯å¢ƒé¢„é…ç½®ä¼˜åŒ–å¥½ï¼›ä»·æ ¼ä½å»‰ï¼Œæ— å‡ºå£æµé‡ ([Lambda GPU Cloud | VM Pricing and Specs](https://lambda.ai/service/gpu-cloud/pricing#:~:text=On,50%20%2F%20GPU%20%2F%20hr))ã€‘ï¼›å¤§æ˜¾å­˜å¡ç§ä¸°å¯Œ | ä¸»è¦åœ¨ç¾å›½åœ°åŒºéƒ¨ç½²ï¼›éœ€æ’é˜Ÿç”³è¯·éƒ¨åˆ†é«˜ç«¯ç®—åŠ›ï¼›æœåŠ¡ç”Ÿæ€ä¸å¦‚AWSå…¨é¢ |

ä¸Šè¿°å¹³å°ä¸­ï¼Œ**AWS/GCP**åŠŸèƒ½å…¨é¢ä½†è´¹ç”¨é«˜æ˜‚ï¼Œæ›´é€‚åˆé¢„ç®—å……è¶³çš„å›¢é˜Ÿï¼›**Paperspace/RunPod/Lambda**ä»·æ ¼äº²æ°‘ï¼Œå°å›¢é˜Ÿå¯ä»¥åˆ©ç”¨å®ƒä»¬æŒ‰éœ€è·å–é«˜ç«¯GPUï¼Œæ¯”å¦‚Lambdaæä¾›çš„48GB A6000ä»… ~$0.8/å° ([Lambda GPU Cloud | VM Pricing and Specs](https://lambda.ai/service/gpu-cloud/pricing#:~:text=On,50%20%2F%20GPU%20%2F%20hr))ã€‘ã€‚åœ¨å®é™…é€‰æ‹©æ—¶ï¼Œåº”è€ƒè™‘ï¼š**ä»·æ ¼/æ€§èƒ½æ¯”**ï¼ˆå¦‚A100æ¯”V100æ–°ä¸€ä»£æ€§èƒ½æ›´å¼ºä½†ä¹Ÿæ›´è´µï¼‰ã€**ä¾¿åˆ©æ€§**ï¼ˆæ˜¯å¦æœ‰ä¸€é”®ç¯å¢ƒé…ç½®ï¼‰ã€**åœ°ç†ä½ç½®**ï¼ˆå»¶è¿Ÿå’Œæ•°æ®åˆè§„ï¼‰ï¼Œä»¥åŠ**ä¸ä¸»æµæ¡†æ¶å…¼å®¹**ï¼ˆä¸Šè¿°å¹³å°åŸºæœ¬éƒ½å…¼å®¹PyTorch/TFï¼Œåªæ˜¯TPUéœ€ç‰¹æ®Šæ”¯æŒï¼‰ã€‚å¯¹äºéœ€è¦é•¿æœŸåå¤è®­ç»ƒçš„åœºæ™¯ï¼Œç§Ÿç”¨æˆ–è´­ä¹°æœ¬åœ°GPUæœåŠ¡å™¨åœ¨é•¿æœŸæˆæœ¬ä¸Šå¯èƒ½æ›´åˆ’ç®—ï¼›ä½†å¯¹çŸ­æœŸé¡¹ç›®ï¼Œç”¨äº‘æœåŠ¡å¿«é€Ÿæ‰©å±•/é‡Šæ”¾èµ„æºèƒ½æé«˜æ•ˆç‡ã€‚

æ€»ä¹‹ï¼Œå°å‹å…¬å¸åº”æ ¹æ®ä»»åŠ¡è§„æ¨¡å¼¹æ€§ä½¿ç”¨èµ„æºï¼šå¼€å‘é˜¶æ®µå¯ç”¨ç¬”è®°æœ¬å°æ¨¡å‹è°ƒè¯•ï¼Œæ­£å¼è®­ç»ƒæ—¶ç§Ÿç”¨äº‘ä¸ŠA100è¿›è¡Œå¾®è°ƒï¼Œéƒ¨ç½²æ—¶å†å°†æ¨¡å‹è’¸é¦åæ”¾åœ¨å°è®¾å¤‡æˆ–ä¾¿å®œäº‘æœºä¸Šï¼Œä»è€Œä»¥æœ€ä½æˆæœ¬å®Œæˆæ•´ä¸ªæµç¨‹ã€‚

## 4. è½¯ä»¶ä¸å·¥å…·é“¾

è¿›è¡Œå°æ¨¡å‹å¾®è°ƒç¦»ä¸å¼€é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¡†æ¶å’Œå·¥å…·é“¾ã€‚ä»¥ä¸‹åˆ—å‡ºå¸¸ç”¨çš„è½¯ä»¶å·¥å…·ï¼Œå¹¶è¯´æ˜å…¶ä½œç”¨åŠå­¦ä¹ èµ„æºï¼š

- **Hugging Face Transformers**: æœ€æµè¡Œçš„NLPæ¨¡å‹åº“ï¼Œæ”¯æŒæ•°åƒç§é¢„è®­ç»ƒæ¨¡å‹çš„åŠ è½½ã€è®­ç»ƒå’Œæ¨ ([ Transformers](https://huggingface.co/docs/transformers/v4.39.0/index#:~:text=State,TensorFlow%2C%20and%20JAX))ã€‘ã€‚å®ƒæä¾›äº†ç»Ÿä¸€çš„APIï¼Œå¯è½»æ¾åœ°ä¸‹è½½å’Œå¾®è°ƒSOTAæ¨¡å‹ï¼Œå¤§å¹…é™ä½ä½¿ç”¨Transformerçš„é—¨ ([ Transformers](https://huggingface.co/docs/transformers/v4.39.0/index#:~:text=State,TensorFlow%2C%20and%20JAX))ã€‘ã€‚å­¦ä¹ èµ„æºï¼šå®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹ï¼ˆåŒ…æ‹¬ä¸­æ–‡æ–‡æ¡£ï¼‰ã€ğŸ¤—å®˜æ–¹è¯¾ç¨‹ ([Transformers - Hugging Face](https://huggingface.co/docs/transformers/v4.39.0/index#:~:text=Transformers%20provides%20APIs%20and%20tools,reduce%20your%20compute%20costs%2C))3ã€‘

- **PEFT (ğŸ¤—** [**P**arameter-**E**fficient **F**ine-**T**uning)**åº“**:  Hugging Faceæ¨å‡ºçš„ä¸“é—¨ç”¨äºå‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ‰©å±•åº“ï¼Œé›†æˆäº†LoRAã€Prefix Tuningã€Adapter ([Comparing the Performance of LLMs: A Deep Dive into RoBERTa, Llama-2, and Mistral-7b for Disaster Tweets Analysis with LoRa | by Mehdi Iraqi | Medium](https://medium.com/@mehdi.iraqui/comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-and-mistral-for-disaster-tweets-8069e717548a#:~:text=PEFT%2C%20Parameter%20Efficient%20Fine,tuning))57ã€‘ã€‚ä½¿ç”¨PEFTï¼Œå¼€å‘è€…å¯ä»¥åœ¨Transformersæ¡†æ¶ä¸‹æ–¹ä¾¿åœ°åº”ç”¨è¿™äº›æ–¹æ³•ï¼Œæ— éœ€ä»é›¶å®ç°è®ºæ–‡ç®—æ³•ã€‚PEFTçš„è®¾è®¡ä½¿å¾—**åªè®­ç»ƒå°‘é‡é™„åŠ å‚æ•°**æˆä¸ºå¯èƒ½ï¼Œå¤§å¹…èŠ‚ ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C%20a%20library%20of%20parameter,convenient%20to%20share%2C%20store%2C%20and))98ã€‘ã€‚å­¦ä¹ èµ„æºï¼šHugging Face PEFTæ–‡æ¡£ã€å®˜æ–¹åšå®¢ç¤ºä¾‹ã€ç¤¾åŒºæ•™ç¨‹ç­‰ã€‚

- **DeepSpeed**: ç”±å¾®è½¯å¼€æºçš„æ·±åº¦å­¦ä¹ è®­ç»ƒä¼˜åŒ–åº“ã€‚DeepSpeedæä¾›äº†åŒ…æ‹¬**ZeROå¹¶è¡Œ**ï¼ˆZero Redundancy Optimizerï¼‰åœ¨å†…çš„ä¸€ç³»åˆ—æŠ€æœ¯ï¼Œä½¿å¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒæ›´é«˜æ•ˆã€‚å®ƒæ”¯æŒå¯¹ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦ã€å‚æ•°çš„åˆ†ç‰‡å’ŒCPU/offloadï¼Œæ˜¾è‘—é™ä½å•GPUæ˜¾å­˜å ç”¨ï¼Œå¹¶æä¾›æµæ°´çº¿å¹¶è¡Œã€å¼ é‡å¹¶è¡Œç­‰æ–¹æ¡ˆã€‚ç®€å•æ¥è¯´ï¼ŒDeepSpeedè®©åˆ†å¸ƒå¼å¤§æ¨¡å‹è®­ç»ƒâ€œå¼€ç®±å³ç”¨â€ï¼Œå¤§å¹…æé«˜äº†è®­ç»ƒé€Ÿåº¦ ([Zero Redundancy Optimizer - DeepSpeed](https://www.deepspeed.ai/tutorials/zero/#:~:text=DeepSpeed%20is%20a%20deep%20learning,training%20easy%2C%20efficient%2C%20and%20effective))L8ã€‘ã€‚å­¦ä¹ èµ„æºï¼šDeepSpeedå®˜æ–¹æ•™ç¨‹ã€HuggingÂ Faceå…³äºä½¿ç”¨DeepSpeed ([DeepSpeed - Hugging Face](https://huggingface.co/docs/accelerate/v0.11.0/en/deepspeed#:~:text=DeepSpeed%20,Below%20is%20a%20short))L7ã€‘ã€‚

- **BitsAndBytes**: ä¸“æ³¨äºä½æ¯”ç‰¹å®½åº¦è®¡ç®—çš„åº“ï¼Œæä¾›äº†**8-bit/4-bité‡åŒ–**çš„é«˜æ•ˆCUDAå†… ([Quantization using bitsandbytes - deepblue research - Medium](https://dbrpl.medium.com/quantization-using-bitsandbytes-f8bbeb6b4576#:~:text=Quantization%20using%20bitsandbytes%20,large%20models%20into%20available))24ã€‘ã€‚é›†æˆTransformersåï¼Œå¯ä¸€è¡Œä»£ç å°†æ¨¡å‹åŠ è½½ä¸º8-bitæƒé‡ï¼Œä»è€Œå°†æ¨ç†æ˜¾å­˜å‡åŠä¸”å‡ ä¹æ— æ€§ ([bitsandbytes - Hugging Face](https://huggingface.co/docs/bitsandbytes/main/en/index#:~:text=LLM.int8%28%29%20or%208,This%20method))L4ã€‘ã€‚bitsandbytesè¿˜å®ç°äº†8-bitä¼˜åŒ–å™¨ï¼Œä½¿åœ¨ä½ç²¾åº¦ä¸‹è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚QLoRAæ–¹æ³•æ­£æ˜¯å»ºç«‹åœ¨bitsandbytesçš„4-bité‡åŒ–æ”¯ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=ecosystem,the%20paper%20is%20as%20follows))88ã€‘ã€‚å­¦ä¹ èµ„æºï¼šHugging Faceåšå®¢ã€Š4-bité‡åŒ–ä¸QLo ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,hours%20of%20finetuning%20on%20a))90ã€‘ã€bitsandbyteså®˜æ–¹GitHubæ–‡æ¡£ç­‰ã€‚

- **OpenLLM**: ç”±BentoMLæ¨å‡ºçš„å¼€æºå¹³å°ï¼Œç”¨äºè¿è¡Œå’Œéƒ¨ç½²å¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚OpenLLMå¯ä»¥è®©å¼€å‘è€…**ç”¨ä¸€è¡Œå‘½ä»¤å¯åŠ¨ä¸€ä¸ªOpenAIé£æ ¼çš„APIæœåŠ¡**æ¥æ‰˜ç®¡æœ¬åœ°çš„ ([bentoml/OpenLLM: Run any open-source LLMs, such as ... - GitHub](https://github.com/bentoml/OpenLLM#:~:text=OpenLLM%20allows%20developers%20to%20run,APIs%20with%20a%20single%20command))L7ã€‘ã€‚å®ƒæ”¯æŒè¯¸å¦‚LLaMAã€GPT-Neoç­‰æ¨¡å‹ï¼Œå¹¶ä¸BentoMLç”Ÿæ€é›†æˆï¼Œæ–¹ä¾¿åœ°è¿›è¡Œæ¨¡å‹å¾®è°ƒã€æœåŠ¡å°è£…å’Œ ([Announcing OpenLLM: An Open-Source Platform for Running Large ...](https://www.bentoml.com/blog/announcing-open-llm-an-open-source-platform-for-running-large-language-models-in-production?ref=dataphoenix.info#:~:text=Announcing%20OpenLLM%3A%20An%20Open,tune%2C%20serve%2C%20deploy%2C))L8ã€‘ã€‚å¯¹äºå°å‹å…¬å¸ï¼ŒOpenLLMé™ä½äº†å°†å¾®è°ƒæ¨¡å‹éƒ¨ç½²ä¸ºåœ¨çº¿æœåŠ¡çš„é—¨æ§›ã€‚å­¦ä¹ èµ„æºï¼šOpenLLMå®˜æ–¹æ–‡æ¡£ã€BentoMLåšå®¢ç³»åˆ—ï¼ˆå¦‚â€œOpenLLM in Actionâ€æ•™ç¨‹ï¼‰ã€‚

- **vLLM**: ä¸“ä¸º**é«˜ååé‡æ¨ç†**ä¼˜åŒ–çš„å¼€æºåº“ã€‚vLLMå¼•å…¥äº†**PagedAttention**æœºåˆ¶ï¼Œæœ‰æ•ˆç®¡ç†å¤§æ¨¡å‹æ¨ç†æ—¶çš„æ˜¾å­˜ï¼Œå¹¶æ”¯æŒåŠ¨æ€æ‰¹å¤„ç†ï¼Œå°†GPUåˆ©ç”¨ç‡å‹ ([Meet vLLM: For faster, more efficient LLM inference and serving](https://www.redhat.com/en/blog/meet-vllm-faster-more-efficient-llm-inference-and-serving#:~:text=With%20the%20need%20for%20LLM,with%20much%20less%C2%A0KV%20cache%20waste))44ã€‘ã€‚å®éªŒæ˜¾ç¤ºï¼ŒvLLMç›¸æ¯”ä¼ ç»Ÿçš„HuggingFace Transformersæˆ–Text Generation Inferenceï¼Œ**ååé‡æå‡å¯è¾¾24å€**ï¼Œæå¤§å‡å°‘äº†KVç¼“ ([Meet vLLM: For faster, more efficient LLM inference and serving](https://www.redhat.com/en/blog/meet-vllm-faster-more-efficient-llm-inference-and-serving#:~:text=aimed%20to%20solve%20some%20of,with%20much%20less%C2%A0KV%20cache%20waste))43ã€‘ã€‚è¿™æ„å‘³ç€å°å…¬å¸ç”¨åŒæ ·çš„ç¡¬ä»¶å¯ä»¥æœåŠ¡æ›´å¤šå¹¶å‘ç”¨æˆ·ã€‚vLLMæ˜“ç”¨æ€§ä¹Ÿå¾ˆå¥½ï¼šæä¾›ç±»ä¼¼APIåŠ è½½æ¨¡å‹ï¼Œå³å¯å¯åŠ¨é«˜é€Ÿæ¨ç†æœåŠ¡ã€‚å­¦ä¹ èµ„æºï¼švLLMå®˜æ–¹GitHubã€è®ºæ–‡â€œEfficient Memory Management for LLM Serving with PagedAttenti ([Meet vLLM: For faster, more efficient LLM inference and serving](https://www.redhat.com/en/blog/meet-vllm-faster-more-efficient-llm-inference-and-serving#:~:text=With%20the%20need%20for%20LLM,with%20much%20less%C2%A0KV%20cache%20waste))44ã€‘ã€ä»¥åŠRedHatæŠ€æœ¯åšå®¢å¯¹vLLM ([Meet vLLM: For faster, more efficient LLM inference and serving](https://www.redhat.com/en/blog/meet-vllm-faster-more-efficient-llm-inference-and-serving#:~:text=aimed%20to%20solve%20some%20of,with%20much%20less%C2%A0KV%20cache%20waste))43ã€‘ã€‚

- **Hugging Face AutoTrain**: Hugging Faceçš„è‡ªåŠ¨åŒ–è®­ç»ƒå¹³å°ã€‚AutoTrainæä¾›**é›¶ä»£ç **ç•Œé¢ï¼Œç”¨æˆ·ä¸Šä¼ æ•°æ®é›†å¹¶é€‰æ‹©æ¨¡å‹ï¼Œå³å¯è‡ªåŠ¨å®Œæˆå¾®è°ƒï¼Œå¹¶éƒ¨ç½²æ¨¡å‹ä¸º ([AutoTrain - Hugging Face](https://huggingface.co/docs/autotrain/v0.6.10/en/index#:~:text=AutoTrain%20,tasks%2C%20and%20for%20Speech))12ã€‘ã€‚å®ƒæ”¯æŒNLPã€CVç­‰å¤šç§ä»»åŠ¡ï¼Œè®©æ²¡æœ‰æ·±åº¦å­¦ä¹ èƒŒæ™¯çš„å›¢é˜Ÿä¹Ÿèƒ½è®­ç»ƒå®š ([AutoTrain - Hugging Face](https://huggingface.co/docs/autotrain/en/index#:~:text=AutoTrain%20,the%20complexities%20of%20model%20training))12ã€‘ã€‚å¯¹äºå°ä¼ä¸šï¼ŒAutoTrainå¯ç”¨äºå¿«é€ŸéªŒè¯å¾®è°ƒæƒ³æ³•æˆ–Baselineå®éªŒã€‚å­¦ä¹ èµ„æºï¼šAutoTrainå®˜æ–¹æ–‡æ¡£ã€Hugging Faceç¤¾åŒºåšå®¢å’Œæ•™ç¨‹ï¼ˆå¦‚KDnuggetsçš„ä½¿ ([How to Use Hugging Face AutoTrain to Fine-tune LLMs - KDnuggets](https://www.kdnuggets.com/how-to-use-hugging-face-autotrain-to-finetune-llms#:~:text=How%20to%20Use%20Hugging%20Face,Vision%2C%20Tabular%2C%20and%20NLP%20tasks))34ã€‘ï¼‰ã€‚

- **Colossal-AI**: é¢å‘å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”± HPC-AI Tech å¼€å‘ã€‚Colossal-AIé›†æˆäº†**å¤šç»´å¹¶è¡Œç­–ç•¥**ï¼ˆæ•°æ®å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€å¼ é‡å¹¶è¡Œç­‰ï¼‰å’Œ**å¼‚æ„å†…å­˜ç®¡ç†**ï¼ˆGPU+CPUæ··åˆå­˜å‚¨ã€å¤§æ‰¹æ¬¡checkpointç­‰ï¼‰ï¼Œæ—¨åœ¨é™ä½å¤§å‹AIæ¨¡å‹è®­ç»ƒçš„ç®—åŠ›å’Œæ˜¾ ([65-billion-parameter large model pretraining accelerated by 38%, best practices for building LLaMA-like base models open-source](https://company.hpc-ai.com/blog/large-model-pretraining#:~:text=Prof,multidimensional%20parallelism%2C%20heterogeneous%20memory%2C%20etc))03ã€‘ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒè®©å¼€å‘è€…å¯ä»¥åƒå†™å•æœºä»£ç ä¸€æ ·è®­ç»ƒåˆ†å¸ƒå¼ ([hpcaitech/ColossalAI: Making large AI models cheaper ... - GitHub](https://github.com/hpcaitech/ColossalAI#:~:text=GitHub%20github.com%20%20Colossal,like%20how%20you%20write))L4ã€‘ã€‚Colossal-AIè¿˜æä¾›è¯¸å¦‚**1-bit Adam**ä¼˜åŒ–å™¨ã€**Gemini**å†…å­˜è°ƒåº¦ç­‰é»‘ç§‘æŠ€ï¼Œåœ¨ä¸šç•Œä¸€äº›åƒå¡è§„æ¨¡è®­ç»ƒä¸­å·² ([65-billion-parameter large model pretraining accelerated by 38%, best practices for building LLaMA-like base models open-source](https://company.hpc-ai.com/blog/large-model-pretraining#:~:text=made%20possible%20by%20Colossal,multidimensional%20parallelism%2C%20heterogeneous%20memory%2C%20etc))03ã€‘ã€‚å°å‹å›¢é˜Ÿå¯åˆ©ç”¨å…¶å¯¹ç°æœ‰ä»£ç çš„åŠ é€Ÿå’Œå¹¶è¡ŒåŒ…è£…ï¼Œåœ¨è¾ƒå°‘GPUä¸Šå°è¯•æ›´å¤§çš„æ¨¡å‹è®­ç»ƒã€‚å­¦ä¹ èµ„æºï¼šColossal-AIå®˜æ–¹æ–‡æ¡£ä¸ç¤ºä¾‹ã€MediumæŠ€æœ¯åš ([Efficient and Easy Training of Large AI Models â€” Introducing Colossal-AI | by HPC-AI Tech | Medium](https://medium.com/@hpcaitech/efficient-and-easy-training-of-large-ai-models-introducing-colossal-ai-ab571176d3ed#:~:text=This%20is%20where%20Colossal,More%20on%20this%20later))95ã€‘ç­‰ã€‚

ä¸Šè¿°å·¥å…·å„æœ‰ä¾§é‡ï¼šTransformers/PEFTæä¾›æ¨¡å‹å’Œç®—æ³•å±‚æ”¯æŒï¼ŒDeepSpeed/Colossalä¾§é‡åˆ†å¸ƒå¼å’Œä½å†…å­˜è®­ç»ƒï¼Œbitsandbytes/vLLMèšç„¦ä½ç²¾åº¦å’Œé«˜æ€§èƒ½æ¨ç†ï¼ŒOpenLLM/AutoTrainç®€åŒ–äº†éƒ¨ç½²å’Œè‡ªåŠ¨åŒ–æµç¨‹ã€‚å»ºè®®æ ¹æ®å›¢é˜Ÿèƒ½åŠ›é€ä¸€æŒæ¡ï¼šä¼˜å…ˆç†Ÿæ‚‰ğŸ¤—Transformers+PEFTè¿›è¡ŒåŸºæœ¬å¾®è°ƒï¼Œç„¶åå­¦ä¹ DeepSpeedæˆ–Colossal-AIæå‡è®­ç»ƒæ•ˆç‡ï¼Œæœ€åæ ¹æ®éƒ¨ç½²éœ€æ±‚å¼•å…¥vLLMæˆ–OpenLLMç­‰ã€‚åœ¨å®è·µä¸­ï¼Œè¿™äº›å·¥å…·å¾€å¾€ç»„åˆä½¿ç”¨ï¼Œä¾‹å¦‚â€œTransformers+PEFTè¿›è¡ŒLoRAè®­ç»ƒï¼Œbitsandbyteså®ç°8-bitåŠ è½½ï¼Œæœ€ç»ˆç”¨vLLMéƒ¨ç½²æœåŠ¡â€ï¼Œå½¢æˆå®Œæ•´é«˜æ•ˆçš„å¾®è°ƒæµæ°´çº¿ã€‚

## 5. ä¸­è‹±æ–‡ä¼˜ç§€æ–‡ç« ä¸è®ºæ–‡æ¨è

å­¦ä¹ å°æ¨¡å‹å¾®è°ƒï¼Œå……åˆ†åˆ©ç”¨ç¤¾åŒºèµ„æºäº‹åŠåŠŸå€ã€‚ä»¥ä¸‹æ•´ç†äº†å€¼å¾—ä¸€è¯»çš„ä¸­è‹±æ–‡æ–‡ç« å’Œè®ºæ–‡ï¼Œå¹¶ç»™å‡ºæ¨èçš„é˜…è¯»é¡ºåºï¼š

### ğŸ“ å­¦æœ¯è®ºæ–‡

1. **ã€ŠAttention Is All You Needã€‹** (Vaswani et al., 2017) â€“ Transformeræ¶æ„å¥ åŸºæ€§è®ºæ–‡ã€‚è™½è¾ƒè‰°æ·±ï¼Œä½†å»ºè®®è‡³å°‘äº†è§£å…¶æ ¸å¿ƒæ€æƒ³ï¼šè‡ªæ³¨æ„åŠ›ã€å¤šå¤´æœºåˆ¶å’Œç¼–ç è§£ç ç»“æ„ã€‚

2. **ã€ŠBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingã€‹** (Devlin et al., 2018) â€“ æå‡ºBERTæ¨¡å‹ï¼Œä»‹ç»äº†Masked LMå’ŒNext Sentence Predictioné¢„è®­ç»ƒä»»åŠ¡ã€‚å¸®åŠ©ç†è§£åŒå‘ç¼–ç å™¨çš„é¢„è®­ç»ƒèŒƒå¼ã€‚

3. **ã€ŠLoRA: Low-Rank Adaptation of Large Language Modelsã€‹** (Hu et al., 2021) â€“ **æ¨èé¦–å…ˆç²¾è¯»**ã€‚LoRAåŸå§‹è®ºæ–‡ï¼Œæå‡ºåœ¨å†»ç»“æƒé‡åŸºç¡€ä¸Šæ³¨å…¥å¯è®­ç»ƒä½ç§©çŸ©é˜µçš„æ–¹æ³•ã€‚è®ºæ–‡è¯¦å®å¯¹æ¯”äº†LoRAä¸å…¨é‡å¾®è°ƒã€Adapterç­‰ï¼Œç»“æœæ˜¾ç¤ºLoRAå°†å¯è®­ç»ƒå‚æ•°å‡å°‘**ä¸‡å€**ä»èƒ½ä¸å…¨å¾® ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=reducing%20the%20number%20of%20trainable,We%20also))-L64ã€‘ã€‚é˜…è¯»æ­¤è®ºæ–‡å¯ä»¥æ·±å…¥ç†è§£å‚æ•°é«˜æ•ˆå¾®è°ƒçš„åŸç†å’Œä¼˜åŠ¿ã€‚

4. **ã€ŠQLoRA: Efficient Finetuning of Quantized LLMsã€‹** (Dettmers et al., 2023) â€“ **LoRAè®ºæ–‡è¯»æ¯•åï¼Œå»ºè®®ç´§æ¥ç€çœ‹QLoRA**ã€‚è¯¥è®ºæ–‡æå‡ºå°†é¢„è®­ç»ƒæ¨¡å‹æƒé‡é‡åŒ–ä¸º4-bitä»¥å¤§å¹…é™ä½æ˜¾å­˜ï¼Œå†ç»“åˆLoRAå¾®è°ƒï¼Œä»è€Œ**åœ¨å•GPUä¸Šå¾®è°ƒ65Bæ¨¡å‹ä¸”æ•ˆæœä¸16-bitå¾® ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,hours%20of%20finetuning%20on%20a))-L90ã€‘ã€‚è®ºæ–‡é™„å¸¦äº†å¤§é‡å®éªŒï¼ˆå¦‚Guanacoæ¨¡å‹è¾¾åˆ°ChatGPT 99 ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,innovations%20to%20save%20memory%20without))-L91ã€‘ï¼‰å’ŒæŠ€å·§ï¼ˆNF4é‡åŒ–æ ¼å¼ã€Double Quantizationç­‰ï¼‰ï¼Œéå¸¸å…·æœ‰å®ç”¨ä»·å€¼ã€‚

5. **ã€ŠLLaMA: Open and Efficient Foundation Language Modelsã€‹** (Touvron et al., 2023) â€“ Metaå‘å¸ƒLLaMAæ¨¡å‹çš„è®ºæ–‡ã€‚å±•ç¤ºäº†ä½¿ç”¨å…¬å¼€æ•°æ®è®­ç»ƒé«˜æ€§èƒ½æ¨¡å‹çš„è·¯å¾„ï¼Œå¹¶ç»™å‡ºLLaMAå„å°ºå¯¸æ¨¡å‹çš„æ€§èƒ½ï¼š**LLaMA-13Bè¶…è¿‡GPT-3ã€65Bé€¼è¿‘GPT- ([[2302.13971] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971#:~:text=,models%20to%20the%20research%20community))-L60ã€‘ã€‚é˜…è¯»è¿™ç¯‡è®ºæ–‡æœ‰åŠ©äºè®¤æ¸…å°å‚æ•°å¤§æ¨¡å‹çš„æ½œåŠ›ï¼Œé¼“èˆå°å›¢é˜Ÿåˆ©ç”¨å¼€æºæ¨¡å‹ã€‚ä¹Ÿå¯é¡ºå¸¦é˜…è¯»LLaMA-2çš„æ›´æ–°ï¼ˆæ‰©å±•ä¸Šä¸‹æ–‡ã€å¼•å…¥å¯¹è¯è°ƒä¼˜ç­‰ï¼‰ã€‚

6. **ï¼ˆå¯é€‰ï¼‰** *DistilBERT, TinyBERT ç­‰è’¸é¦ç›¸å…³è®ºæ–‡* â€“ å¦‚æœ‰ä½™åŠ›ï¼Œå¯¹æ¨¡å‹å‹ç¼©æœ‰å…´è¶£ï¼Œå¯é˜…è¯»Sanhç­‰äººçš„DistilBERTè®ºæ–‡ï¼ˆ ([[PDF] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and ...](https://arxiv.org/pdf/1910.01108#:~:text=,of%20the%20language%20understanding))-L29ã€‘ä»¥åŠHintonçš„çŸ¥è¯†è’¸é¦å¼€åˆ›æ€§è®ºæ–‡ï¼ˆ2015ï¼‰ã€‚è¿™äº›è®ºæ–‡è§£é‡Šäº†å¦‚ä½•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å°†æ¨¡å‹å°ºå¯¸å‹ç¼©åˆ°åŸæ¥çš„å¾ˆå°æ¯”ä¾‹ï¼Œå¯¹äºéƒ¨ç½²ä¼˜åŒ–å¾ˆæœ‰å¯å‘ã€‚

### ğŸ“– è‹±æ–‡åšå®¢/æ•™ç¨‹

- **Hugging Face Transformers å®˜æ–¹æ•™ç¨‹** â€“ åˆå­¦è€…å‹å¥½çš„å…¥é—¨èµ„æºï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ç­‰ä»»åŠ¡çš„å¾®è°ƒå®ä¾‹ã€‚é…å¥—çš„[å®˜æ–¹è¯¾ç¨‹è§†é¢‘](https://huggingface.co/learn/nlp-course)æ·±å…¥æµ…å‡ºè®²è§£TransformeråŸç†å’Œä½¿ç”¨æ–¹æ³•ã€‚

- **Sebastian Raschkaã€ŠFinetuning Pretrained Transformersã€‹** â€“ ã€ŠAhead-of-AIã€‹æ‚å¿—ä¸­çš„ä¸€ç¯‡é€šä¿—æ–‡ç« ï¼Œæ¦‚æ‹¬äº†ä½¿ç”¨é¢„è®­ç»ƒTransformerçš„ä¸‰ç§æ–¹å¼ï¼šç‰¹å¾æå–ã€å¾® ([Using and Finetuning Pretrained Transformers - Ahead of AI](https://magazine.sebastianraschka.com/p/using-and-finetuning-pretrained-transformers#:~:text=Using%20and%20Finetuning%20Pretrained%20Transformers,subset%20of%20the%20model))0-L44ã€‘ã€‚é€‚åˆåˆå­¦è€…å»ºç«‹å¯¹å¾®è°ƒæ•´ä½“æ€è·¯çš„è®¤è¯†ã€‚

- **Medium åšå®¢: *â€œFine-Tuning LLMs in 4-bit with QLoRAâ€*** â€“ ä½œè€… Luke Monington ç”¨ç”ŸåŠ¨çš„è¯­è¨€ä»‹ç»QLoRAåŸç†å’Œå®æ“æ­¥éª¤ï¼Œå¸®åŠ©å¿«é€Ÿä¸Šæ‰‹4 ([QLoRA â€” How to Fine-Tune an LLM on a Single GPU - Medium](https://medium.com/data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#:~:text=QLoRA%20%E2%80%94%20How%20to%20Fine,to%20do%20this%20without))9-L17ã€‘ã€‚

- **Medium åšå®¢: *â€œLLM Fine-Tuning with LoRAâ€*** â€“ ä½œè€… Kedar Naik ç”¨é€šä¿—è¯­è¨€è§£é‡ŠLoRAçš„å·¥ä½œåŸç†ï¼Œå¹¶æä¾›åŸºäºğŸ¤—PE ([LoRA: Low-Rank Adaptation of Large Language Models - GitHub](https://github.com/microsoft/LoRA#:~:text=LoRA%3A%20Low,while%20freezing%20the%20original%20weights))13-L21ã€‘ã€‚é˜…è¯»æ­¤æ–‡å¯ä»¥åŠ æ·±å¯¹LoRAä½ç§©åˆ†è§£ç›´è§‚å«ä¹‰çš„ç†è§£ã€‚

- **Lightning AI Blog: *â€œFinetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experimentsâ€*** â€“ è¿™æ˜¯ä¸€ç¯‡ç»éªŒåˆ†äº«æ€§è´¨çš„é•¿æ–‡ï¼Œæ€»ç»“äº†ä½œè€…è¿›è¡Œæ•°ç™¾æ¬¡LoRA/QLoRAå®éªŒå¾—åˆ°çš„å®ç”¨è§è§£ã€‚æ¶µç›–å¦‚ä½•é€‰æ‹©åŸºç¡€æ¨¡å‹ã€LoRAè¶…å‚æ•°å¯¹æ€§èƒ½çš„å½±å“ã€QLoRAçš„ä¼˜åŠ£ç­‰ï¼Œéå¸¸å…· ([ä½¿ç”¨LoRAå’ŒQLoRAå¾®è°ƒLLMsï¼šæ•°ç™¾æ¬¡å®éªŒçš„è§è§£-CSDNåšå®¢](https://blog.csdn.net/qq_20144897/article/details/136105160#:~:text=Image%20%E6%9C%AC%E6%96%87%E5%9B%B4%E7%BB%95%E4%BD%BF%E7%94%A8LoRA%E5%92%8CQLoRA%E5%BE%AE%E8%B0%83%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%B1%95%E5%BC%80%EF%BC%8C%E4%BB%8B%E7%BB%8D%E8%AF%84%E4%BC%B0%E4%BB%BB%E5%8A%A1%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E5%9F%BA%E4%BA%8ELit%20,%E3%80%82))26-L34ã€‘ã€‚ï¼ˆæœ‰ä¸­æ–‡è¯‘æ–‡ï¼Œè§ä¸‹ï¼‰

- **Chris McCormick åšå®¢: *â€œQLoRA and 4-bit Quantizationâ€*** â€“ å¯¹QLoRAè¿›è¡Œé€šä¿—è§£é‡Šçš„ç³»åˆ—æ–‡ç« ï¼Œä»åŸºç¡€çš„8-bité‡åŒ–è®²èµ·ï¼Œå†åˆ°4-bitå®ç°ç»†èŠ‚ï¼Œé€‚åˆåœ¨è¯»è¿‡QLoRAè®ºæ–‡åæŸ¥æ¼è¡¥ç¼ºï¼ŒåŠ æ·±å¯¹å…¶ä¸­æŠ€æœ¯ç‚¹ï¼ˆå¦‚NF4é‡åŒ–ï¼‰çš„ç†è§£ã€‚

### ğŸ“‘ ä¸­æ–‡ä¼˜è´¨å†…å®¹

- **çŸ¥ä¹ä¸“æ ã€ŠLLMå¾®è°ƒçŸ¥å¤šå°‘ã€‹ç³»åˆ—**ï¼ˆä½œè€…ï¼šåŒ—æ–¹çš„éƒ ç­‰ï¼‰ â€“ ç³»ç»Ÿæ€§ä»‹ç»å¤§æ¨¡å‹å¾®è°ƒæŠ€æœ¯çš„ç³»åˆ—æ–‡ç« ã€‚å…¶ä¸­**ç¬¬5ç¯‡**èšç„¦LoRAã€AdaLoRA ([å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆäº”ï¼‰-LoRAã€AdaLoRAã€QLoRA](https://zhuanlan.zhihu.com/p/636215898#:~:text=%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E7%BB%BC%E8%BF%B0%EF%BC%88%E4%BA%94%EF%BC%89))7â€ L5-L8ã€‘ï¼›**å®æˆ˜ç¯‡**æ¼”ç¤ºäº†ä½¿ç”¨å•å¡å¯¹ChatGLM-6Bè¿›è¡ŒQL ([ä½¿ç”¨LoRA å’ŒQLoRA å¾®è°ƒLLMçš„æ·±å…¥æŒ‡å— - çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/690739797#:~:text=%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E5%85%B6%E4%BB%96%E6%B5%81%E8%A1%8C%E6%8A%80%E6%9C%AF%E3%80%82))â€ L9-L13ã€‘ã€‚è¯­è¨€å¹³å®ï¼Œé€‚åˆæœ‰ä¸€å®šåŸºç¡€åæ·±å…¥å­¦ä¹ å‚æ•°é«˜æ•ˆå¾®è°ƒçš„å®ç°ç»†èŠ‚ã€‚

- **çŸ¥ä¹ä¸“æ ã€Šä½¿ç”¨LoRAå’ŒQLoRAå¾®è°ƒLLMçš„æ·±å…¥æŒ‡å—ã€‹**ï¼ˆä½œè€…ï¼šThoughtsCoderï¼‰ â€“ ä»èƒŒæ™¯ã€æŠ€æœ¯åŸç†åˆ°å…·ä½“å®ç°ï¼Œå…¨æ–¹ä½è®²è§£LoRAå’ŒQLoRAï¼Œåœ¨ç†è®ºè®²è§£çš„åŒæ—¶ä¹Ÿ ([ä½¿ç”¨LoRA å’ŒQLoRA å¾®è°ƒLLMçš„æ·±å…¥æŒ‡å— - çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/690739797#:~:text=%E4%BD%BF%E7%94%A8LoRA%20%E5%92%8CQLoRA%20%E5%BE%AE%E8%B0%83LLM%E7%9A%84%E6%B7%B1%E5%85%A5%E6%8C%87%E5%8D%97%20,%E8%BF%99%E5%B0%B1%E6%98%AFLoRA%20%E7%AD%89PEFT%20%E6%8A%80%E6%9C%AF%E7%9A%84%E7%94%A8%E6%AD%A6%E4%B9%8B%E5%9C%B0%EF%BC%8C%E4%B8%8E%E5%AE%8C%E5%85%A8%E5%BE%AE%E8%B0%83%E7%9B%B8%E6%AF%94%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%8A%80%E6%9C%AF%E5%8F%AF%E4%BB%A5%E8%AE%A9%E6%82%A8%E6%9B%B4%E6%9C%89%E6%95%88%E5%9C%B0%E8%AE%AD%E7%BB%83%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B%E3%80%82%E5%9C%A8%E6%9C%AC%E5%8D%9A%E5%AE%A2%E4%B8%AD%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%86%E4%BB%8B%E7%BB%8DLoRA%E3%80%81QLoRA%20%E4%BB%A5%E5%8F%8A%E4%B8%93%E9%97%A8%E4%BB%8ELoRA%20%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E5%85%B6%E4%BB%96%E6%B5%81%E8%A1%8C%E6%8A%80%E6%9C%AF%E3%80%82))â€ L7-L10ã€‘ã€‚å¯¹è‹±æ–‡èµ„æ–™æ¶ˆåŒ–å›°éš¾çš„è¯»è€…ï¼Œè¿™æ˜¯ä¸€ç¯‡éš¾å¾—çš„ä¸­æ–‡è¯¦è§£æ–‡ç« ã€‚

- **Bç«™è§†é¢‘ã€Šå‡ ç™¾æ¬¡å¤§æ¨¡å‹LoRAå’ŒQLoRAå¾®è°ƒå®è·µçš„ç»éªŒåˆ†äº«ã€‹**ï¼ˆUPä¸»ï¼šå°å·¥èšåˆ›å§‹äººï¼‰ â€“ æ—¶é•¿çº¦1å°æ—¶çš„åˆ†äº«ï¼ŒUPä¸»ç»“åˆè‡ªèº«ä¸°å¯Œå®éªŒï¼Œä»å·¥ç¨‹å®æˆ˜è§’åº¦èŠå¦‚ä½•è®¾ç½®LoRAå„é¡¹è¶…å‚ã€é€‰æ‹©è®­ç»ƒç­–ç•¥é¿å…è¸©å‘ç­‰ï¼ˆå¯¹åº”å‰è¿°Lightningæ–‡ç« çš„ä¸­æ–‡æ¼”ç»ï¼‰ã€‚è§†é¢‘å¼¹å¹•äº’åŠ¨ä¹Ÿæä¾› ([å‡ ç™¾æ¬¡å¤§æ¨¡å‹LoRAå’ŒQLoRA å¾®è°ƒå®è·µçš„ç»éªŒåˆ†äº«](https://www.bilibili.com/video/BV16u4y1a7MH/#:~:text=%E5%87%A0%E7%99%BE%E6%AC%A1%E5%A4%A7%E6%A8%A1%E5%9E%8BLoRA%E5%92%8CQLoRA%20%E5%BE%AE%E8%B0%83%E5%AE%9E%E8%B7%B5%E7%9A%84%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%20%E5%87%A0%E7%99%BE%E6%AC%A1%E5%A4%A7%E6%A8%A1%E5%9E%8BLoRA%E5%92%8CQLoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E8%B7%B5%E7%9A%84%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%2C%20%E8%A7%86%E9%A2%91%E6%92%AD%E6%94%BE%E9%87%8F18230%E3%80%81%E5%BC%B9%E5%B9%95%E9%87%8F6%E3%80%81%E7%82%B9%E8%B5%9E%E6%95%B0547%E3%80%81%E6%8A%95%E7%A1%AC%E5%B8%81%E6%9E%9A%E6%95%B0279%E3%80%81%E6%94%B6%E8%97%8F%E4%BA%BA%E6%95%B01765%E3%80%81%E8%BD%AC%E5%8F%91%E4%BA%BA%E6%95%B0244%2C%20%E8%A7%86%E9%A2%91%E4%BD%9C%E8%80%85%E5%B0%8F%E5%B7%A5%E8%9A%81%E5%88%9B%E5%A7%8B%E4%BA%BA%2C,))L23-L30ã€‘ã€‚

- **Bç«™è§†é¢‘ã€ŠChatGLM-6Bå¾®è°ƒå…¨è§£æï¼šP-Tuning vs LoRAã€‹** â€“ æ¸…åçš„ChatGLMå›¢é˜Ÿå®˜æ–¹å‡ºå“ï¼Œè¯¦ç»†ä»‹ç»äº†GLMæ¨¡å‹çš„å¾®è°ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬P-Tuningå’ŒLoRAä¸¤ ([ã€å®˜æ–¹æ•™ç¨‹ã€‘ChatGLM-6B å¾®è°ƒï¼šP-Tuningï¼ŒLoRAï¼ŒFull parameter](https://www.bilibili.com/video/BV1fd4y1Z7Y5/#:~:text=%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B%20ChatGLM))L15-L22ã€‘ã€‚é€šè¿‡å…·ä½“æ¡ˆä¾‹ï¼ˆä¸­æ–‡å¯¹è¯æ¨¡å‹å¾®è°ƒï¼‰ï¼ŒåŠ æ·±å¯¹ä¸åŒå¾®è°ƒæŠ€æœ¯æ•ˆæœå·®å¼‚çš„è®¤è¯†ã€‚

- **CSDNåšå®¢ã€ŠPhi-2 å°æ¨¡å‹çš„QLoRAå¾®è°ƒæ•™ç¨‹ã€‹** â€“ ä»¥å¾®è½¯Phi-2 2.7Bæ¨¡å‹ä¸ºä¾‹ï¼Œä»ç¯å¢ƒé…ç½®ã€ä»£ç æ­¥éª¤åˆ°æ•ˆæœå±•ç¤ºï¼Œå®Œæ•´è®°å½•äº†åœ¨Kaggleç¬”è®°æœ¬ä¸Šä½¿ç”¨ä¸¤å¼ T4æ˜¾å¡è¿›è¡Œå¯¹è¯æ‘˜ ([Phi-2å°è¯­è¨€æ¨¡å‹QLoRAå¾®è°ƒæ•™ç¨‹_è‡ªå·±è®­ç»ƒphi-2-CSDNåšå®¢](https://blog.csdn.net/qq_20144897/article/details/135459060#:~:text=%E5%B0%B1%E5%9C%A8%E4%B8%8D%E4%B9%85%E5%89%8D%EF%BC%8C%E5%BE%AE%E8%BD%AF%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83%E4%BA%86%E4%B8%80%E4%B8%AA27%20%E4%BA%BF%E5%8F%82%E6%95%B0%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94Phi))5â€ L1-L4ã€‘ã€‚è¿™ç¯‡æ•™ç¨‹éå¸¸è´´è¿‘å®æˆ˜ï¼Œå±•ç¤ºäº†åœ¨æœ‰é™GPUèµ„æºä¸‹å¾®è°ƒä¸­æ–‡å¯¹è¯æ¨¡å‹çš„**å…·ä½“æŠ€å·§**ï¼Œå¯¹åˆå­¦è€…å…·æœ‰å‚è€ƒä»·å€¼ã€‚

ä»¥ä¸Šèµ„æºå»ºè®®æŒ‰å…ˆæ˜“åéš¾é¡ºåºæ¶ˆåŒ–ï¼šå…ˆé˜…è¯»åŸºç¡€åšå®¢/æ•™ç¨‹æ‰“å¥½æ¦‚å¿µåŸºç¡€ï¼Œå†ç ”è¯»å­¦æœ¯è®ºæ–‡ç†è§£åŸç†ç»†èŠ‚ï¼Œæœ€åç»“åˆå®è·µç±»æ–‡ç« /è§†é¢‘è¿›è¡Œå®æ“è®­ç»ƒã€‚**ä¼˜å…ˆ**å…³æ³¨LoRAå’ŒQLoRAç›¸å…³çš„å†…å®¹ï¼Œå®ƒä»¬æ˜¯å°æ¨¡å‹å¾®è°ƒçš„é‡ä¸­ä¹‹é‡ï¼›åŒæ—¶æŠ½æ—¶é—´ç ”è¯»LLaMAç­‰æ¨¡å‹è®ºæ–‡ä»¥äº†è§£å®è§‚å›¾æ™¯ã€‚é€šè¿‡â€œç†è®º+å®æˆ˜â€åŒç®¡é½ä¸‹ï¼Œèƒ½æ›´å…¨é¢åœ°æŒæ¡å¾®è°ƒæŠ€å·§ã€‚

## 6. å­¦ä¹ è·¯å¾„å»ºè®®

é’ˆå¯¹ä»é›¶å¼€å§‹çš„å°å‹å›¢é˜Ÿï¼Œå»ºè®®æŒ‰ç…§ç”±æµ…å…¥æ·±çš„é˜¶æ®µæ€§è·¯çº¿å­¦ä¹ å°æ¨¡å‹å¾®è°ƒæŠ€æœ¯ã€‚æ¯ä¸ªé˜¶æ®µè®¾å®šæ˜ç¡®çš„ç›®æ ‡ï¼Œå¹¶é…ä»¥å®è·µç»ƒä¹ ï¼š

**é˜¶æ®µ1ï¼šæŒæ¡åŸºç¡€ç†è®ºä¸å·¥å…·ä½¿ç”¨**  
*ç›®æ ‡*: ç†è§£Transformeræ¶æ„åŸºæœ¬åŸç†ï¼Œç†Ÿæ‚‰ä¸»æµé¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠèƒ½å¤Ÿä½¿ç”¨ç°æœ‰æ¡†æ¶è·‘é€šç®€å•ç¤ºä¾‹ã€‚  
*å†…å®¹*: å­¦ä¹ Transformerè®ºæ–‡æ ¸å¿ƒæ¦‚å¿µï¼ˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€ç¼–ç å™¨vsè§£ç å™¨ï¼‰ã€äº†è§£BER ([How is the bidirectional context achieved in BERT?](https://ai.stackexchange.com/questions/46054/how-is-the-bidirectional-context-achieved-in-bert#:~:text=How%20is%20the%20bidirectional%20context,i%E2%88%921%20when%20predicting%20token%20i))L13-L16ã€‘ã€‚æµè§ˆğŸ¤—Transformerså®˜æ–¹æ•™ç¨‹ï¼Œè¿è¡Œä¸€ä¸ªç°æˆçš„æ–‡æœ¬åˆ†ç±»demoï¼ˆå¦‚å¾®è°ƒBERTåšæƒ…æ„Ÿåˆ†æï¼‰ã€‚  
*å®è·µ*: åœ¨æœ¬åœ°ç”µè„‘æˆ–Colabä¸Šï¼Œä½¿ç”¨Hugging FaceåŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹ï¼Œå¯¹å°æ•°æ®é›†è¿›è¡Œå¾®è°ƒå’ŒéªŒè¯ã€‚è§‚å¯Ÿå…¨é‡å¾®è°ƒçš„æ•ˆæœå’Œå¯èƒ½é‡åˆ°çš„è¿‡æ‹Ÿåˆç°è±¡ï¼Œç»ƒä¹ ä½¿ç”¨Trainer APIè®¾ç½®è®­ç»ƒå‚æ•°ã€‚  
*äº§å‡º*: èƒ½å¤Ÿç‹¬ç«‹å®Œæˆä¸€ä¸ªåŸºç¡€NLPä»»åŠ¡çš„å¾®è°ƒæµç¨‹ï¼Œå¯¹Transformerå®¶æ—æ¨¡å‹æœ‰åˆæ­¥ç›´è§‚è®¤è¯†ã€‚

**é˜¶æ®µ2ï¼šæ·±å…¥å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•**  
*ç›®æ ‡*: æŒæ¡LoRAã€Adapterç­‰PEFTæ–¹æ³•çš„åŸç†ä¸ä½¿ç”¨ï¼Œç† ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=reducing%20the%20number%20of%20trainable,We%20also))â€ L59-L64ã€‘ã€‚  
*å†…å®¹*: é˜…è¯»LoRAè®ºæ–‡æˆ–ç›¸å…³åšå®¢ï¼Œå¼„æ¸…ä½ç§©åˆ† ([[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685#:~:text=reducing%20the%20number%20of%20trainable,We%20also))â€ L59-L64ã€‘ï¼›äº†è§£Adapteræ’å…¥å±‚çš„åšæ³•å’Œä¸LoRAçš„åŒºåˆ«ã€‚å­¦ä¹ ğŸ¤—PEFTåº“çš„ä½¿ç”¨æ–¹æ³•ï¼Œé€šè¿‡å®˜æ–¹ç¤ºä¾‹ç†Ÿæ‚‰è°ƒç”¨æ¥å£ã€‚  
*å®è·µ*: é€‰æ‹©ä¸€ä¸ªä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚DistilBERTæˆ–GPT-2ï¼‰ä½œä¸ºåŸºåº•ï¼Œåœ¨ç›¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸Šåˆ†åˆ«å°è¯•å…¨é‡å¾®è°ƒå’ŒLoRAå¾®è°ƒã€‚å¯¹æ¯”ä¸¤è€…åœ¨æ˜¾å­˜å ç”¨ã€æ”¶æ•›é€Ÿåº¦ã€æœ€ç»ˆæ•ˆæœä¸Šçš„å·®å¼‚ã€‚ä¾‹å¦‚ï¼Œç”¨IMDBæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼Œåˆ†åˆ«å…¨é‡å¾®è°ƒBERTå’Œåº”ç”¨LoRAå¾®è°ƒBERTï¼Œè®°å½•æ˜¾å­˜å’Œç²¾åº¦å˜åŒ–ã€‚  
*äº§å‡º*: èƒ½ä½¿ç”¨PEFTåº“ä¸ºTransformeræ¨¡å‹æ·»åŠ LoRAé€‚é…å™¨å¹¶å®Œæˆè®­ç»ƒã€‚æ’°å†™ä¸€æ®µå¯¹æ¯”æŠ¥å‘Šï¼Œè¯´æ˜LoRAå¦‚ä½•ä»¥è¾ƒå°‘å‚æ•°è¾¾åˆ°æ¥è¿‘å…¨å¾®è°ƒçš„æ•ˆæœï¼Œæœ‰åŠ©äºåŠ æ·±ç†è§£ã€‚

**é˜¶æ®µ3ï¼šæŒ‘æˆ˜æ›´å¤§æ¨¡å‹çš„å¾®è°ƒ**  
*ç›®æ ‡*: å­¦ä¹ ä½¿ç”¨è¿›é˜¶æŠ€å·§åœ¨æœ‰é™ç¡¬ä»¶ä¸Šå¾®è°ƒæ•°åäº¿å‚æ•°æ¨¡å‹ï¼ŒæŒæ¡QLoRAã€8- ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=match%20at%20L199%20enables%2033B,on%20a%20single%2046GB%20GPU))9â€ L11-L16ã€‘ã€‚  
*å†…å®¹*: å­¦ä¹ BitsAndBytesé‡åŒ–åŸç†ï¼Œé‡ç‚¹ç ”è¯»QLoRAè®ºæ–‡/åšå®¢ä¸­å…³äº4-bité‡ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,hours%20of%20finetuning%20on%20a))4â€ L83-L90ã€‘ã€‚é…ç½®ç¯å¢ƒå®‰è£…bitsandbytesã€DeepSpeedç­‰å·¥å…·ã€‚äº†è§£DeepSpeed ZeROæ‹†åˆ† ([Zero Redundancy Optimizer - DeepSpeed](https://www.deepspeed.ai/tutorials/zero/#:~:text=DeepSpeed%20is%20a%20deep%20learning,training%20easy%2C%20efficient%2C%20and%20effective))ã€‚  
*å®è·µ*: åœ¨ä¸€å°é…å¤‡**å•å¼ GPUï¼ˆå¦‚24GBï¼‰**çš„æœºå™¨ä¸Šï¼Œå°è¯•å¯¹7B~13Bé‡çº§çš„å¼€æºæ¨¡å‹è¿›è¡ŒQLoRAå¾®è°ƒã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨LLaMA-7Bæˆ–Mistral-7Bæ¨¡å‹ï¼Œå¯¹ä¸€ä¸ªå°å‹å¯¹è¯æ•°æ®é›†åšæŒ‡ä»¤å¾®è°ƒï¼šå¯ç”¨4-bité‡åŒ–åŠ è½½æ¨¡å‹ï¼Œåº”ç”¨LoRAé€‚é…å™¨ï¼Œä½¿ç”¨PEFT + DeepSpeed ZerO Stage 2ç»„åˆä»¥é™ä½å†…å­˜ã€‚ç›‘æ§æ˜¾å­˜åˆ©ç”¨ç‡ï¼Œç¡®ä¿è¿‡ç¨‹åœ¨å•GPUè·‘é€šã€‚è®­ç»ƒå®Œæˆåï¼Œä¸åŸå§‹æ¨¡å‹è¿›è¡Œå¯¹è¯æµ‹è¯•ï¼Œä½“ä¼šå¾®è°ƒæå‡ã€‚  
*äº§å‡º*: æˆåŠŸåœ¨æœ¬åœ°å•å¡ä¸Šå¾®è°ƒå‡ºä¸€ä¸ªåŠŸèƒ½å‹å°æ¨¡å‹ï¼ˆå¦‚ç®€æ˜“å®¢æœbotï¼‰ã€‚ç¼–å†™è®­ç»ƒç¬”è®°ï¼Œè®°å½•QLoRAé…ç½®ç»†èŠ‚ã€è¸©å‘ï¼ˆä¾‹å¦‚å¯èƒ½é‡åˆ°çš„NaNæ¢¯åº¦ã€é˜²æ­¢æº¢å‡ºçš„æŠ€å·§ç­‰ï¼‰ï¼Œä¸ºåç»­å›¢é˜Ÿæˆå‘˜æä¾›å€Ÿé‰´ã€‚

**é˜¶æ®µ4ï¼šæ¨¡å‹å‹ç¼©ä¸éƒ¨ç½²**  
*ç›®æ ‡*: å°†å¾®è°ƒåçš„æ¨¡å‹ä¼˜åŒ–ä¸ºå¯åœ¨ç”Ÿäº§ç¯å¢ƒé«˜æ•ˆè¿è¡Œçš„ç‰ˆæœ¬ï¼Œå¹¶æŒæ¡éƒ¨ç½²ä¸Šçº¿çš„æ–¹æ³•ã€‚  
*å†…å®¹*: å­¦ä¹ çŸ¥è¯†è’¸é¦çš„æ–¹æ³•è®ºï¼Œå‚è€ƒDistilBERTç­‰æ¡ˆä¾‹äº†è§£è’¸ ([[PDF] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and ...](https://arxiv.org/pdf/1910.01108#:~:text=,of%20the%20language%20understanding))7â€ L21-L29ã€‘ã€‚è°ƒç ”å¸¸ç”¨çš„æ¨¡å‹éƒ¨ç½²æ¡†æ¶ï¼Œå¦‚FastAPI/Flaskæ­é…Transformer pipelinesç®€å•éƒ¨ç½²ï¼Œä»¥åŠBentoML/OpenL ([bentoml/OpenLLM: Run any open-source LLMs, such as ... - GitHub](https://github.com/bentoml/OpenLLM#:~:text=OpenLLM%20allows%20developers%20to%20run,APIs%20with%20a%20single%20command))ã€‚äº†è§£vLLMç­‰é«˜æ€§èƒ½æ¨ç†å¼•æ“çš„ä½¿ç”¨ï¼Œå°¤ ([Meet vLLM: For faster, more efficient LLM inference and serving](https://www.redhat.com/en/blog/meet-vllm-faster-more-efficient-llm-inference-and-serving#:~:text=aimed%20to%20solve%20some%20of,with%20much%20less%C2%A0KV%20cache%20waste))L439-L443ã€‘ã€‚  
*å®è·µ*: é’ˆå¯¹é˜¶æ®µ3å¾—åˆ°çš„å¾®è°ƒæ¨¡å‹ï¼Œæ‰§è¡Œä¸€æ¬¡**è’¸é¦**å®éªŒï¼šä»¥å¾®è°ƒæ¨¡å‹ä¸ºæ•™å¸ˆï¼Œé€‰ç”¨ä¸€ä¸ªè¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ï¼ˆä¾‹å¦‚ç”¨LLaMA-7Bæ•™å¸ˆï¼Œè’¸é¦åˆ°3Bé‡çº§çš„å°æ¨¡å‹æˆ–ç²¾ç®€ç‰ˆï¼Œæ¯”å¦‚Alpaca-LoRA-7Bè’¸é¦åˆ°ä¸€ä¸ªMiniLMï¼‰ã€‚å‡†å¤‡å°‘é‡æœªæ ‡æ³¨è¯­æ–™ï¼Œè®©æ•™å¸ˆæ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾è®­ç»ƒå­¦ç”Ÿã€‚è¯„ä¼°å­¦ç”Ÿæ¨¡å‹åœ¨å…³é”®ä»»åŠ¡ä¸Šçš„æ€§èƒ½æŸå¤±æƒ…å†µã€‚æ¥ç€ï¼Œé€‰æ‹©æ€§èƒ½è¶³å¤Ÿçš„æ¨¡å‹ç‰ˆæœ¬ï¼Œè¿›è¡Œéƒ¨ç½²ï¼šä½¿ç”¨**OpenLLM**å°†æ¨¡å‹åŒ…è£…æˆä¸€ä¸ª ([Announcing OpenLLM: An Open-Source Platform for Running Large ...](https://www.bentoml.com/blog/announcing-open-llm-an-open-source-platform-for-running-large-language-models-in-production?ref=dataphoenix.info#:~:text=Announcing%20OpenLLM%3A%20An%20Open,tune%2C%20serve%2C%20deploy%2C))ï¼Œæˆ–é‡‡ç”¨**vLLM**æ‰˜ç®¡æ¨¡å‹ä»¥æµ‹è¯•é«˜å¹¶å‘ ([Meet vLLM: For faster, more efficient LLM inference and serving](https://www.redhat.com/en/blog/meet-vllm-faster-more-efficient-llm-inference-and-serving#:~:text=aimed%20to%20solve%20some%20of,with%20much%20less%C2%A0KV%20cache%20waste))L439-L443ã€‘ã€‚å¯ä»¥åœ¨æœ¬åœ°Dockeræˆ–äº‘ä¸»æœºä¸Šéƒ¨ç½²ï¼Œå¹¶ç¼–å†™ç®€å•å‰ç«¯æ¼”ç¤ºã€‚  
*äº§å‡º*: ä¸€ä¸ªå¯é€šè¿‡APIè®¿é—®çš„å¾®è°ƒæ¨¡å‹æœåŠ¡ï¼Œä»¥åŠä¸€ä»½éƒ¨ç½²æ–‡æ¡£ï¼ˆåŒ…æ‹¬ç¯å¢ƒä¾èµ–ã€å¯åŠ¨å‘½ä»¤ã€å‹æµ‹ç»“æœç­‰ï¼‰ã€‚å›¢é˜Ÿåº”èƒ½æ®æ­¤å°†å¾®è°ƒæ¨¡å‹é›†æˆåˆ°è‡ªæœ‰äº§å“ï¼ˆå¦‚æ¨èç³»ç»Ÿåç«¯ã€èŠå¤©æœºå™¨äººæœåŠ¡ï¼‰ä¸­ã€‚

**é˜¶æ®µ5ï¼šæŒç»­æ”¹è¿›å’Œæ‹“å±•**  
*ç›®æ ‡*: è·Ÿè¿›è¡Œä¸šæœ€æ–°è¿›å±•ï¼Œä¸æ–­ä¼˜åŒ–å¾®è°ƒæµç¨‹ï¼Œå¹¶æ¢ç´¢æ›´å¤šå‚ç›´åœºæ™¯çš„åº”ç”¨ã€‚  
*å†…å®¹*: å®šæœŸç ”è¯»æ–°å‘è¡¨çš„è®ºæ–‡æˆ–åšå®¢ï¼ˆä¾‹å¦‚æ–°çš„PEFTæ–¹æ³•å¦‚Adapters-XL, RAPIDç­‰ï¼‰ã€‚å…³æ³¨å¼€æºç¤¾åŒºé¡¹ç›®ï¼Œå¦‚HuggingFace Hubä¸Šçš„æœ€æ–°7B/13Bæ¨¡å‹ï¼Œåœ¨æœ¬é¢†åŸŸæ•°æ®ä¸Šçš„å¾®è°ƒè¡¨ç°ã€‚ç»“åˆä¸šåŠ¡éœ€æ±‚ï¼Œå°è¯•ä¸åŒä»»åŠ¡ï¼šå¦‚å°†å¾®è°ƒæŠ€æœ¯ç”¨äº**å¤šæ¨¡æ€**ï¼ˆè§†è§‰ã€è¯­éŸ³ï¼‰æ¨¡å‹ã€å°æ ·æœ¬å­¦ä¹ ç­‰æ–¹å‘ï¼Œä¸°å¯Œå›¢é˜ŸæŠ€èƒ½å›¾è°±ã€‚  
*å®è·µ*: å¯ä»¥ä¸¾åŠå›¢é˜Ÿå†…éƒ¨çš„åˆ†äº«ä¼šï¼Œæ¯äººè·Ÿè¸ªä¸€ç¯‡æ–°è®ºæ–‡æˆ–å·¥å…·å¹¶åšæ€»ç»“ã€‚åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œä¸æ–­è¿­ä»£ï¼šä¾‹å¦‚ä¸Šçº¿æ¨¡å‹åï¼Œæ ¹æ®ç”¨æˆ·åé¦ˆæ•°æ®ç»§ç»­å¾®è°ƒï¼ˆonline learningï¼‰æˆ–è€…è’¸é¦åˆ°ç§»åŠ¨ç«¯ã€‚å°è¯•ä½¿ç”¨AutoTrainç­‰AutoMLå·¥å…·åŠ é€Ÿéƒ¨åˆ†æ¨¡å‹é€‰æ‹©æˆ–è¶…å‚è°ƒèŠ‚æµç¨‹ï¼Œæå‡ç ”å‘æ•ˆç‡ã€‚  
*äº§å‡º*: å›¢é˜Ÿå½¢æˆä¸€å¥—é€‚åˆè‡ªèº«ä¸šåŠ¡çš„å°æ¨¡å‹å¾®è°ƒ**æµæ°´çº¿**å’Œ**æœ€ä½³å®è·µ**æ–‡æ¡£ï¼Œä»æ•°æ®å‡†å¤‡â†’å¾®è°ƒâ†’è¯„ä¼°â†’éƒ¨ç½²å„ç¯èŠ‚éƒ½æœ‰æ¸…æ™°æŒ‡å¼•ã€‚è¿™å¥—ç§¯ç´¯å°†æˆä¸ºå°å‹å…¬å¸å®è´µçš„AIèµ„äº§ï¼Œæ”¯æŒæœªæ¥æ›´å¤šåº”ç”¨è½åœ°ã€‚

æŒ‰ç…§ä»¥ä¸Šé˜¶æ®µé€æ­¥å­¦ä¹ å¹¶å®è·µï¼Œå›¢é˜Ÿæˆå‘˜å°†ä»é›¶èµ·æ­¥ï¼Œæœ€ç»ˆæŒæ¡å°å‹AIæ¨¡å‹å¾®è°ƒçš„å…¨æµç¨‹ã€‚æœ‰äº†æ‰å®çš„åŸºç¡€å’Œç»éªŒï¼Œå°å‹å…¬å¸ä¹Ÿèƒ½çµæ´»è¿ç”¨å°æ¨¡å‹ï¼Œåœ¨æ¨èç³»ç»Ÿã€SaaSæœåŠ¡ç­‰å‚ç›´é¢†åŸŸæ‰“é€ å‡ºå±äºè‡ªå·±çš„æ™ºèƒ½åº”ç”¨ï¼ 

